<h1>第三章 大语言模型基础</h1>
<p>前两章分别介绍了智能体的定义和发展历史，本章将完全聚焦于大语言模型本身解答一个关键问题：现代智能体是如何工作的？我们将从语言模型的基本定义出发，通过对这些原理的学习，为理解LLM如何获得强大的知识储备与推理能力打下坚实的基础。</p>
<h2>3.1 语言模型与 Transformer 架构</h2>
<h3>3.1.1 从 N-gram 到 RNN</h3>
<p><strong>语言模型 (Language Model, LM)</strong> 是自然语言处理的核心，其根本任务是计算一个词序列（即一个句子）出现的概率。一个好的语言模型能够告诉我们什么样的句子是通顺的、自然的。在多智能体系统中，语言模型是智能体理解人类指令、生成回应的基础。本节将回顾从经典的统计方法到现代深度学习模型的演进历程，为理解后续的 Transformer 架构打下坚实的基础。</p>
<p><strong>（1）统计语言模型与N-gram的思想</strong></p>
<p>在深度学习兴起之前，统计方法是语言模型的主流。其核心思想是，一个句子出现的概率，等于该句子中每个词出现的条件概率的连乘。对于一个由词 $w_1,w_2,\cdots,w_m$ 构成的句子 S，其概率 P(S) 可以表示为：</p>
<p>$$P(S)=P(w_1,w_2,…,w_m)=P(w_1)⋅P(w_2∣w_1)⋅P(w_3∣w_1,w_2)⋯P(w_m∣w_1,…,w_{m−1})$$</p>
<p>这个公式被称为概率的链式法则。然而，直接计算这个公式几乎是不可能的，因为像 $P(w_m∣w_1,\cdots,w_{m−1})$ 这样的条件概率太难从语料库中估计了，词序列 $w_1,\cdots,w_{m−1}$ 可能从未在训练数据中出现过。</p>
<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-0.png" alt="图片描述" width="90%">
  <p>图 3.1 马尔可夫假设示意图</p>
</div>
<p>为了解决这个问题，研究者引入了<strong>马尔可夫假设 (Markov Assumption)</strong> 。其核心思想是：我们不必回溯一个词的全部历史，可以近似地认为，一个词的出现概率只与它前面有限的 $n−1$ 个词有关，如图3.1所示。基于这个假设建立的语言模型，我们称之为 <strong>N-gram模型</strong>。这里的 "N" 代表我们考虑的上下文窗口大小。让我们来看几个最常见的例子来理解这个概念：</p>
<ul>
<li><strong>Bigram (当 N=2 时)</strong> ：这是最简单的情况，我们假设一个词的出现只与它前面的一个词有关。因此，链式法则中复杂的条件概率 $P(w_i∣w_1,\cdots,w_{i−1})$ 就可以被近似为更容易计算的形式：</li>
</ul>
<p>$$P(w_{i}∣w_{1},…,w_{i−1})≈P(w_{i}∣w_{i−1})$$</p>
<ul>
<li><strong>Trigram (当 N=3 时)</strong> ：类似地，我们假设一个词的出现只与它前面的两个词有关：</li>
</ul>
<p>$$P(w_i∣w_1,…,w_{i−1})≈P(w_i∣w_{i−2},w_{i−1})$$</p>
<p>这些概率可以通过在大型语料库中进行<strong>最大似然估计(Maximum Likelihood Estimation,MLE)</strong> 来计算。这个术语听起来很复杂，但其思想非常直观：最可能出现的，就是我们在数据中看到次数最多的。例如，对于 Bigram 模型，我们想计算在词 $w_{i−1}$ 出现后，下一个词是 $w_i$ 的概率 $P(w_i∣w_{i−1})$。根据最大似然估计，这个概率可以通过简单的计数来估算：</p>
<p>$$P(w_i∣w_{i−1})=\frac{Count(w_{i−1},w_i)}{Count(w_{i−1})}$$</p>
<p>这里的 <code>Count()</code> 函数就代表“计数”：</p>
<ul>
<li>$Count(w_i−1,w_i)$：表示词对 $(w_{i−1},w_i)$ 在语料库中连续出现的总次数。</li>
<li>$Count(w_{i−1})$：表示单个词 $w_{i−1}$ 在语料库中出现的总次数。</li>
</ul>
<p>公式的含义就是：我们用“词对 $Count(w_i−1,w_i)$ 出现的次数”除以“词 $Count(w_{i−1})$ 出现的总次数”，来作为 $P(w_i∣w_{i−1})$ 的一个近似估计。</p>
<p>为了让这个过程更具体，我们来手动进行一次计算。假设我们拥有一个仅包含以下两句话的迷你语料库：<code>datawhale agent learns</code>, <code>datawhale agent works</code>。我们的目标是：使用 Bigram (N=2) 模型，估算句子 <code>datawhale agent learns</code> 出现的概率。根据 Bigram 的假设，我们每次会考察连续的两个词（即一个词对）。</p>
<p><strong>第一步：计算第一个词的概率</strong> $P(datawhale)$ 这是 <code>datawhale</code> 出现的次数除以总词数。<code>datawhale</code> 出现了 2 次，总词数是 6。</p>
<p>$$P(\text{datawhale}) = \frac{\text{总语料中"datawhale"的数量}}{\text{总语料的词数}} = \frac{2}{6} \approx 0.333$$</p>
<p><strong>第二步：计算条件概率</strong> $P(agent∣datawhale)$ 这是词对 <code>datawhale agent</code> 出现的次数除以 <code>datawhale</code> 出现的总次数。<code>datawhale agent</code> 出现了 2 次，<code>datawhale</code> 出现了 2 次。</p>
<p>$$P(\text{agent}|\text{datawhale}) =  \frac{\text{Count}(\text{datawhale agent})}{\text{Count}(\text{datawhale})} =  \frac{2}{2} = 1$$</p>
<p><strong>第三步：计算条件概率</strong> $P(learns∣agent)$ 这是词对 <code>agent learns</code> 出现的次数除以 <code>agent</code> 出现的总次数。<code>agent learns</code> 出现了 1 次，<code>agent</code> 出现了 2 次。</p>
<p>$$P(\text{learns}|\text{agent}) =  \frac{\text{Count(agent learns)}}{\text{Count(agent)}} =  \frac{1}{2} = 0.5$$</p>
<p><strong>最后：将概率连乘</strong> 所以，整个句子的近似概率为：</p>
<p>$$P(\text{datawhale agent learns}) \approx  P(\text{datawhale}) \cdot  P(\text{agent}|\text{datawhale}) \cdot  P(\text{learns}|\text{agent}) \approx  0.333 \cdot 1 \cdot 0.5 \approx 0.167$$</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">import</span> collections
</span><span class="code-line line-number" line="2">
</span><span class="code-line line-number" line="3"><span class="token comment"># 示例语料库，与上方案例讲解中的语料库保持一致</span>
</span><span class="code-line line-number" line="4">corpus <span class="token operator">=</span> <span class="token string">"datawhale agent learns datawhale agent works"</span>
</span><span class="code-line line-number" line="5">tokens <span class="token operator">=</span> corpus<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="6">total_tokens <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="7">
</span><span class="code-line line-number" line="8"><span class="token comment"># --- 第一步:计算 P(datawhale) ---</span>
</span><span class="code-line line-number" line="9">count_datawhale <span class="token operator">=</span> tokens<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">'datawhale'</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="10">p_datawhale <span class="token operator">=</span> count_datawhale <span class="token operator">/</span> total_tokens
</span><span class="code-line line-number" line="11"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"第一步: P(datawhale) = </span><span class="token interpolation"><span class="token punctuation">{</span>count_datawhale<span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>total_tokens<span class="token punctuation">}</span></span><span class="token string"> = </span><span class="token interpolation"><span class="token punctuation">{</span>p_datawhale<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="12">
</span><span class="code-line line-number" line="13"><span class="token comment"># --- 第二步:计算 P(agent|datawhale) ---</span>
</span><span class="code-line line-number" line="14"><span class="token comment"># 先计算 bigrams 用于后续步骤</span>
</span><span class="code-line line-number" line="15">bigrams <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> tokens<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="16">bigram_counts <span class="token operator">=</span> collections<span class="token punctuation">.</span>Counter<span class="token punctuation">(</span>bigrams<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="17">count_datawhale_agent <span class="token operator">=</span> bigram_counts<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'datawhale'</span><span class="token punctuation">,</span> <span class="token string">'agent'</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="18"><span class="token comment"># count_datawhale 已在第一步计算</span>
</span><span class="code-line line-number" line="19">p_agent_given_datawhale <span class="token operator">=</span> count_datawhale_agent <span class="token operator">/</span> count_datawhale
</span><span class="code-line line-number" line="20"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"第二步: P(agent|datawhale) = </span><span class="token interpolation"><span class="token punctuation">{</span>count_datawhale_agent<span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>count_datawhale<span class="token punctuation">}</span></span><span class="token string"> = </span><span class="token interpolation"><span class="token punctuation">{</span>p_agent_given_datawhale<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="21">
</span><span class="code-line line-number" line="22"><span class="token comment"># --- 第三步:计算 P(learns|agent) ---</span>
</span><span class="code-line line-number" line="23">count_agent_learns <span class="token operator">=</span> bigram_counts<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'agent'</span><span class="token punctuation">,</span> <span class="token string">'learns'</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="24">count_agent <span class="token operator">=</span> tokens<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">'agent'</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="25">p_learns_given_agent <span class="token operator">=</span> count_agent_learns <span class="token operator">/</span> count_agent
</span><span class="code-line line-number" line="26"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"第三步: P(learns|agent) = </span><span class="token interpolation"><span class="token punctuation">{</span>count_agent_learns<span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>count_agent<span class="token punctuation">}</span></span><span class="token string"> = </span><span class="token interpolation"><span class="token punctuation">{</span>p_learns_given_agent<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="27">
</span><span class="code-line line-number" line="28"><span class="token comment"># --- 最后:将概率连乘 ---</span>
</span><span class="code-line line-number" line="29">p_sentence <span class="token operator">=</span> p_datawhale <span class="token operator">*</span> p_agent_given_datawhale <span class="token operator">*</span> p_learns_given_agent
</span><span class="code-line line-number" line="30"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"最后: P('datawhale agent learns') ≈ </span><span class="token interpolation"><span class="token punctuation">{</span>p_datawhale<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> * </span><span class="token interpolation"><span class="token punctuation">{</span>p_agent_given_datawhale<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> * </span><span class="token interpolation"><span class="token punctuation">{</span>p_learns_given_agent<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> = </span><span class="token interpolation"><span class="token punctuation">{</span>p_sentence<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="31">
</span><span class="code-line line-number" line="32"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
</span><span class="code-line line-number" line="33">第一步<span class="token punctuation">:</span> P<span class="token punctuation">(</span>datawhale<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">6</span> <span class="token operator">=</span> <span class="token number">0.333</span>
</span><span class="code-line line-number" line="34">第二步<span class="token punctuation">:</span> P<span class="token punctuation">(</span>agent<span class="token operator">|</span>datawhale<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">2</span> <span class="token operator">=</span> <span class="token number">1.000</span>
</span><span class="code-line line-number" line="35">第三步<span class="token punctuation">:</span> P<span class="token punctuation">(</span>learns<span class="token operator">|</span>agent<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span><span class="token number">2</span> <span class="token operator">=</span> <span class="token number">0.500</span>
</span><span class="code-line line-number" line="36">最后<span class="token punctuation">:</span> P<span class="token punctuation">(</span><span class="token string">'datawhale agent learns'</span><span class="token punctuation">)</span> ≈ <span class="token number">0.333</span> <span class="token operator">*</span> <span class="token number">1.000</span> <span class="token operator">*</span> <span class="token number">0.500</span> <span class="token operator">=</span> <span class="token number">0.167</span>
</span></code></pre>
<p>N-gram 模型虽然简单有效，但有两个致命缺陷：</p>
<ol>
<li><strong>数据稀疏性 (Sparsity)</strong> ：如果一个词序列从未在语料库中出现，其概率估计就为 0，这显然是不合理的。虽然可以通过平滑 (Smoothing) 技术缓解，但无法根除。</li>
<li><strong>泛化能力差：</strong>模型无法理解词与词之间的语义相似性。例如，即使模型在语料库中见过很多次 <code>agent learns</code>，它也无法将这个知识泛化到语义相似的词上。当我们计算 <code>robot learns</code> 的概率时，如果 <code>robot</code> 这个词从未出现过，或者 <code>robot learns</code> 这个组合从未出现过，模型计算出的概率也会是零。模型无法理解 <code>agent</code> 和 <code>robot</code> 在语义上的相似性。</li>
</ol>
<p><strong>（2）神经网络语言模型与词嵌入</strong></p>
<p>N-gram 模型的根本缺陷在于它将词视为孤立、离散的符号。为了克服这个问题，研究者们转向了神经网络，并提出了一种思想：用连续的向量来表示词。2003年，Bengio 等人提出的<strong>前馈神经网络语言模型 (Feedforward Neural Network Language Model)</strong> 是这一领域的里程碑<sup>[1]</sup>。</p>
<p>其核心思想可以分为两步：</p>
<ol>
<li><strong>构建一个语义空间</strong>：创建一个高维的连续向量空间，然后将词汇表中的每个词都映射为该空间中的一个点。这个点（即向量）就被称为<strong>词嵌入 (Word Embedding)</strong> 或词向量。在这个空间里，语义上相近的词，它们对应的向量在空间中的位置也相近。例如，<code>agent</code> 和 <code>robot</code> 的向量会靠得很近，而 <code>agent</code> 和 <code>apple</code> 的向量会离得很远。</li>
<li><strong>学习从上下文到下一个词的映射</strong>：利用神经网络的强大拟合能力，来学习一个函数。这个函数的输入是前 $n−1$ 个词的词向量，输出是词汇表中每个词在当前上下文后出现的概率分布。</li>
</ol>
<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-1.png" alt="图片描述" width="90%">
  <p>图 3.2 神经网络语言模型架构示意图</p>
</div>
<p>如图3.2所示，在这个架构中，词嵌入是在模型训练过程中自动学习得到的。模型为了完成“预测下一个词”这个任务，会不断调整每个词的向量位置，最终使这些向量能够蕴含丰富的语义信息。一旦我们将词转换成了向量，我们就可以用数学工具来度量它们之间的关系。最常用的方法是<strong>余弦相似度 (Cosine Similarity)</strong> ，它通过计算两个向量夹角的余弦值来衡量它们的相似性。</p>
<p>$$\text{similarity}(\vec{a}, \vec{b}) = \cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}| |\vec{b}|}$$</p>
<p>这个公式的含义是：</p>
<ul>
<li>如果两个向量方向完全相同，夹角为0°，余弦值为1，表示完全相关。</li>
<li>如果两个向量方向正交，夹角为90°，余弦值为0，表示毫无关系。</li>
<li>如果两个向量方向完全相反，夹角为180°，余弦值为-1，表示完全负相关。</li>
</ul>
<p>通过这种方式，词向量不仅能捕捉到“同义词”这类简单的关系，还能捕捉到更复杂的类比关系。</p>
<p>一个著名的例子展示了词向量捕捉到的语义关系： <code>vector('King') - vector('Man') + vector('Woman')</code> 这个向量运算的结果，在向量空间中与 <code>vector('Queen')</code> 的位置惊人地接近。这好比在进行语义的平移：我们从“国王”这个点出发，减去“男性”的向量，再加上“女性”的向量，最终就抵达了“女王”的位置。这证明了词嵌入能够学习到“性别”、“皇室”这类抽象概念。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
</span><span class="code-line line-number" line="2">
</span><span class="code-line line-number" line="3"><span class="token comment"># 假设我们已经学习到了简化的二维词向量</span>
</span><span class="code-line line-number" line="4">embeddings <span class="token operator">=</span> <span class="token punctuation">{</span>
</span><span class="code-line line-number" line="5">    <span class="token string">"king"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
</span><span class="code-line line-number" line="6">    <span class="token string">"queen"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
</span><span class="code-line line-number" line="7">    <span class="token string">"man"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
</span><span class="code-line line-number" line="8">    <span class="token string">"woman"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="9"><span class="token punctuation">}</span>
</span><span class="code-line line-number" line="10">
</span><span class="code-line line-number" line="11"><span class="token keyword">def</span> <span class="token function">cosine_similarity</span><span class="token punctuation">(</span>vec1<span class="token punctuation">,</span> vec2<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="12">    dot_product <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>vec1<span class="token punctuation">,</span> vec2<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="13">    norm_product <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>vec1<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>vec2<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="14">    <span class="token keyword">return</span> dot_product <span class="token operator">/</span> norm_product
</span><span class="code-line line-number" line="15">
</span><span class="code-line line-number" line="16"><span class="token comment"># king - man + woman</span>
</span><span class="code-line line-number" line="17">result_vec <span class="token operator">=</span> embeddings<span class="token punctuation">[</span><span class="token string">"king"</span><span class="token punctuation">]</span> <span class="token operator">-</span> embeddings<span class="token punctuation">[</span><span class="token string">"man"</span><span class="token punctuation">]</span> <span class="token operator">+</span> embeddings<span class="token punctuation">[</span><span class="token string">"woman"</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="18">
</span><span class="code-line line-number" line="19"><span class="token comment"># 计算结果向量与 "queen" 的相似度</span>
</span><span class="code-line line-number" line="20">sim <span class="token operator">=</span> cosine_similarity<span class="token punctuation">(</span>result_vec<span class="token punctuation">,</span> embeddings<span class="token punctuation">[</span><span class="token string">"queen"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="21">
</span><span class="code-line line-number" line="22"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"king - man + woman 的结果向量: </span><span class="token interpolation"><span class="token punctuation">{</span>result_vec<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="23"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"该结果与 'queen' 的相似度: </span><span class="token interpolation"><span class="token punctuation">{</span>sim<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="24">
</span><span class="code-line line-number" line="25"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
</span><span class="code-line line-number" line="26">king <span class="token operator">-</span> man <span class="token operator">+</span> woman 的结果向量<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0.9</span> <span class="token number">0.2</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="27">该结果与 <span class="token string">'queen'</span> 的相似度<span class="token punctuation">:</span> <span class="token number">1.0000</span>
</span></code></pre>
<p>神经网络语言模型通过词嵌入，成功解决了 N-gram 模型的泛化能力差的问题。然而，它仍然有一个类似 N-gram 的限制：上下文窗口是固定的。它只能考虑固定数量的前文，这为能处理任意长序列的循环神经网络埋下了伏笔。</p>
<p><strong>（3）循环神经网络 (RNN) 与长短时记忆网络 (LSTM)</strong></p>
<p>前一节的神经网络语言模型虽然引入了词嵌入解决了泛化问题，但它和 N-gram 模型一样，上下文窗口是固定大小的。为了预测下一个词，它只能看到前 n−1 个词，再早的历史信息就被丢弃了。这显然不符合我们人类理解语言的方式。为了打破固定窗口的限制，<strong>循环神经网络 (Recurrent Neural Network, RNN)</strong> 应运而生，其核心思想非常直观：为网络增加“记忆”能力<sup>[2]</sup>。</p>
<p>如图3.3所示，RNN 的设计引入了一个<strong>隐藏状态 (hidden state)</strong> 向量，我们可以将其理解为网络的短期记忆。在处理序列的每一步，网络都会读取当前的输入词，并结合它上一刻的记忆（即上一个时间步的隐藏状态），然后生成一个新的记忆（即当前时间步的隐藏状态）传递给下一刻。这个循环往复的过程，使得信息可以在序列中不断向后传递。</p>
<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-2.png" alt="图片描述" width="90%">
  <p>图 3.3 RNN 结构示意图</p>
</div>
<p>然而，标准的 RNN 在实践中存在一个严重的问题：<strong>长期依赖问题 (Long-term Dependency Problem)</strong> 。在训练过程中，模型需要通过反向传播算法根据输出端的误差来调整网络深处的权重。对于 RNN 而言，序列的长度就是网络的深度。当序列很长时，梯度在从后向前传播的过程中会经过多次连乘，这会导致梯度值快速趋向于零（<strong>梯度消失</strong>）或变得极大（<strong>梯度爆炸</strong>）。梯度消失使得模型无法有效学习到序列早期信息对后期输出的影响，即难以捕捉长距离的依赖关系。</p>
<p>为了解决长期依赖问题，<strong>长短时记忆网络 (Long Short-Term Memory, LSTM)</strong> 被设计出来<sup>[3]</sup>。LSTM 是一种特殊的 RNN，其核心创新在于引入了<strong>细胞状态 (Cell State)</strong> 和一套精密的<strong>门控机制 (Gating Mechanism)</strong> 。细胞状态可以看作是一条独立于隐藏状态的信息通路，允许信息在时间步之间更顺畅地传递。门控机制则是由几个小型神经网络构成，它们可以学习如何有选择地让信息通过，从而控制细胞状态中信息的增加与移除。这些门包括：</p>
<ul>
<li><strong>遗忘门 (Forget Gate)</strong>：决定从上一时刻的细胞状态中丢弃哪些信息。</li>
<li><strong>输入门 (Input Gate)</strong>：决定将当前输入中的哪些新信息存入细胞状态。</li>
<li><strong>输出门 (Output Gate)</strong>：决定根据当前的细胞状态，输出哪些信息到隐藏状态。</li>
</ul>
<h3>3.1.2 Transformer 架构解析</h3>
<p>在上一节中，我们看到RNN及LSTM通过引入循环结构来处理序列数据，这在一定程度上解决了捕捉长距离依赖的问题。然而，这种循环的计算方式也带来了新的瓶颈：它必须按顺序处理数据。第 t 个时间步的计算，必须等待第 t−1 个时间步完成后才能开始。这意味着 RNN 无法进行大规模的并行计算，在处理长序列时效率低下，这极大地限制了模型规模和训练速度的提升。Transformer在2017 年由谷歌团队提出<sup>[4]</sup>。它完全抛弃了循环结构，转而完全依赖一种名为<strong>注意力 (Attention)</strong> 的机制来捕捉序列内的依赖关系，从而实现了真正意义上的并行计算。</p>
<p><strong>（1）Encoder-Decoder 整体结构</strong></p>
<p>最初的 Transformer 模型是为端到端任务机器翻译而设计的。如图3.4所示，它在宏观上遵循了一个经典的<strong>编码器-解码器 (Encoder-Decoder)</strong> 架构。</p>
<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-3.png" alt="图片描述" width="50%">
  <p>图 3.4 Transformer 整体架构图</p>
</div>
<p>我们可以将这个结构理解为一个分工明确的团队：</p>
<ol>
<li><strong>编码器 (Encoder)</strong> ：任务是“<strong>理解</strong>”输入的整个句子。它会读取所有输入词元(这个概念会在3.2.2节介绍)，最终为每个词元生成一个富含上下文信息的向量表示。</li>
<li><strong>解码器 (Decoder)</strong> ：任务是“<strong>生成</strong>”目标句子。它会参考自己已经生成的前文，并“咨询”编码器的理解结果，来生成下一个词。</li>
</ol>
<p>为了真正理解 Transformer 的工作原理，最好的方法莫过于亲手实现它。在本节中，我们将采用一种“自顶向下”的方法：首先，我们搭建出 Transformer 完整的代码框架，定义好所有需要的类和方法。然后，我们将像完成拼图一样，逐一实现这些类的具体功能。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">import</span> torch
</span><span class="code-line line-number" line="2"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
</span><span class="code-line line-number" line="3"><span class="token keyword">import</span> math
</span><span class="code-line line-number" line="4">
</span><span class="code-line line-number" line="5"><span class="token comment"># --- 占位符模块，将在后续小节中实现 ---</span>
</span><span class="code-line line-number" line="6">
</span><span class="code-line line-number" line="7"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="8">    <span class="token triple-quoted-string string">"""
</span></span><span class="code-line line-number" line="9"><span class="token triple-quoted-string string">    位置编码模块
</span></span><span class="code-line line-number" line="10"><span class="token triple-quoted-string string">    """</span>
</span><span class="code-line line-number" line="11">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="12">        <span class="token keyword">pass</span>
</span><span class="code-line line-number" line="13">
</span><span class="code-line line-number" line="14"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="15">    <span class="token triple-quoted-string string">"""
</span></span><span class="code-line line-number" line="16"><span class="token triple-quoted-string string">    多头注意力机制模块
</span></span><span class="code-line line-number" line="17"><span class="token triple-quoted-string string">    """</span>
</span><span class="code-line line-number" line="18">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="19">        <span class="token keyword">pass</span>
</span><span class="code-line line-number" line="20">
</span><span class="code-line line-number" line="21"><span class="token keyword">class</span> <span class="token class-name">PositionWiseFeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="22">    <span class="token triple-quoted-string string">"""
</span></span><span class="code-line line-number" line="23"><span class="token triple-quoted-string string">    位置前馈网络模块
</span></span><span class="code-line line-number" line="24"><span class="token triple-quoted-string string">    """</span>
</span><span class="code-line line-number" line="25">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="26">        <span class="token keyword">pass</span>
</span><span class="code-line line-number" line="27">
</span><span class="code-line line-number" line="28"><span class="token comment"># --- 编码器核心层 ---</span>
</span><span class="code-line line-number" line="29">
</span><span class="code-line line-number" line="30"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="31">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="32">        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="33">        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 待实现</span>
</span><span class="code-line line-number" line="34">        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> PositionWiseFeedForward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 待实现</span>
</span><span class="code-line line-number" line="35">        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="36">        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="37">        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="38">
</span><span class="code-line line-number" line="39">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="40">        <span class="token comment"># 残差连接与层归一化将在 3.1.2.4 节中详细解释</span>
</span><span class="code-line line-number" line="41">        <span class="token comment"># 1. 多头自注意力</span>
</span><span class="code-line line-number" line="42">        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="43">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="44">
</span><span class="code-line line-number" line="45">        <span class="token comment"># 2. 前馈网络</span>
</span><span class="code-line line-number" line="46">        ff_output <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="47">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>ff_output<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="48">
</span><span class="code-line line-number" line="49">        <span class="token keyword">return</span> x
</span><span class="code-line line-number" line="50">
</span><span class="code-line line-number" line="51"><span class="token comment"># --- 解码器核心层 ---</span>
</span><span class="code-line line-number" line="52">
</span><span class="code-line line-number" line="53"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="54">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="55">        <span class="token builtin">super</span><span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="56">        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 待实现</span>
</span><span class="code-line line-number" line="57">        self<span class="token punctuation">.</span>cross_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 待实现</span>
</span><span class="code-line line-number" line="58">        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> PositionWiseFeedForward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 待实现</span>
</span><span class="code-line line-number" line="59">        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="60">        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="61">        self<span class="token punctuation">.</span>norm3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="62">        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="63">
</span><span class="code-line line-number" line="64">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="65">        <span class="token comment"># 1. 掩码多头自注意力 (对自己)</span>
</span><span class="code-line line-number" line="66">        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="67">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="68">
</span><span class="code-line line-number" line="69">        <span class="token comment"># 2. 交叉注意力 (对编码器输出)</span>
</span><span class="code-line line-number" line="70">        cross_attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>cross_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="71">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>cross_attn_output<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="72">
</span><span class="code-line line-number" line="73">        <span class="token comment"># 3. 前馈网络</span>
</span><span class="code-line line-number" line="74">        ff_output <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="75">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm3<span class="token punctuation">(</span>x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>ff_output<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="76">
</span><span class="code-line line-number" line="77">        <span class="token keyword">return</span> x
</span></code></pre>
<p><strong>（2）从自注意力到多头注意力</strong></p>
<p>现在，我们来填充骨架中最关键的模块，注意力机制。</p>
<p>想象一下我们阅读这个句子：“The agent learns because <strong>it</strong> is intelligent.”。当我们读到加粗的 "<strong>it</strong>" 时，为了理解它的指代，我们的大脑会不自觉地将更多的注意力放在前面的 "agent" 这个词上。<strong>自注意力 (Self-Attention)</strong> 机制就是对这种现象的数学建模。它允许模型在处理序列中的每一个词时，都能兼顾句子中的所有其他词，并为这些词分配不同的“注意力权重”。权重越高的词，代表其与当前词的关联性越强，其信息也应该在当前词的表示中占据更大的比重。</p>
<p>为了实现上述过程，自注意力机制为每个输入的词元向量引入了三个可学习的角色：</p>
<ul>
<li><strong>查询 (Query, Q)</strong>：代表当前词元，它正在主动地“查询”其他词元以获取信息。</li>
<li><strong>键 (Key, K)</strong>：代表句子中可被查询的词元“标签”或“索引”。</li>
<li><strong>值 (Value, V)</strong>：代表词元本身所携带的“内容”或“信息”。</li>
</ul>
<p>这三个向量都是由原始的词嵌入向量乘以三个不同的、可学习的权重矩阵 ($W^Q,W^K,W^V$) 得到的。整个计算过程可以分为以下几步，我们可以把它想象成一次高效的开卷考试：</p>
<ul>
<li>准备“考题”和“资料”：对于句子中的每个词，都通过权重矩阵生成其$Q,K,V$向量。</li>
<li>计算相关性得分：要计算词$A$的新表示，就用词$A$的$Q$向量，去和句子中所有词（包括$A$自己）的$K$向量进行点积运算。这个得分反映了其他词对于理解词$A$的重要性。</li>
<li>稳定化与归一化：将得到的所有分数除以一个缩放因子$\sqrt{d_{k}}$（$d_{k}$是$K$向量的维度），以防止梯度过小，然后用Softmax函数将分数转换成总和为1的权重，也就是归一化的过程。</li>
<li>加权求和：将上一步得到的权重分别乘以每个词对应的$V$向量，然后将所有结果相加。最终得到的向量，就是词$A$融合了全局上下文信息后的新表示。</li>
</ul>
<p>这个过程可以用一个简洁的公式来概括：</p>
<p>$$\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$$</p>
<p>如果只进行一次上述的注意力计算（即单头），模型可能会只学会关注一种类型的关联。比如，在处理 "it" 时，可能只学会了关注主语。但语言中的关系是复杂的，我们希望模型能同时关注多种关系（如指代关系、时态关系、从属关系等）。多头注意力机制应运而生。它的思想很简单：把一次做完变成分成几组，分开做，再合并。</p>
<p>它将原始的 Q, K, V 向量在维度上切分成 h 份（h 就是“头”数），每一份都独立地进行一次单头注意力的计算。这就好比让 h 个不同的“专家”从不同的角度去审视句子，每个专家都能捕捉到一种不同的特征关系。最后，将这 h 个专家的“意见”（即输出向量）拼接起来，再通过一个线性变换进行整合，就得到了最终的输出。</p>
<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-4.png" alt="图片描述" width="50%">
  <p>图 3.5 多头注意力机制</p>
</div>
<p>如图3.5所示，这种设计让模型能够共同关注来自不同位置、不同表示子空间的信息，极大地增强了模型的表达能力。以下是多头注意力的简单实现可供参考。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="2">    <span class="token triple-quoted-string string">"""
</span></span><span class="code-line line-number" line="3"><span class="token triple-quoted-string string">    多头注意力机制模块
</span></span><span class="code-line line-number" line="4"><span class="token triple-quoted-string string">    """</span>
</span><span class="code-line line-number" line="5">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="6">        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="7">        <span class="token keyword">assert</span> d_model <span class="token operator">%</span> num_heads <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">"d_model 必须能被 num_heads 整除"</span>
</span><span class="code-line line-number" line="8">
</span><span class="code-line line-number" line="9">        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model
</span><span class="code-line line-number" line="10">        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
</span><span class="code-line line-number" line="11">        self<span class="token punctuation">.</span>d_k <span class="token operator">=</span> d_model <span class="token operator">//</span> num_heads
</span><span class="code-line line-number" line="12">
</span><span class="code-line line-number" line="13">        <span class="token comment"># 定义 Q, K, V 和输出的线性变换层</span>
</span><span class="code-line line-number" line="14">        self<span class="token punctuation">.</span>W_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="15">        self<span class="token punctuation">.</span>W_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="16">        self<span class="token punctuation">.</span>W_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="17">        self<span class="token punctuation">.</span>W_o <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="18">
</span><span class="code-line line-number" line="19">    <span class="token keyword">def</span> <span class="token function">scaled_dot_product_attention</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="20">        <span class="token comment"># 1. 计算注意力得分 (QK^T)</span>
</span><span class="code-line line-number" line="21">        attn_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="22">
</span><span class="code-line line-number" line="23">        <span class="token comment"># 2. 应用掩码 (如果提供)</span>
</span><span class="code-line line-number" line="24">        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="25">            <span class="token comment"># 将掩码中为 0 的位置设置为一个非常小的负数，这样 softmax 后会接近 0</span>
</span><span class="code-line line-number" line="26">            attn_scores <span class="token operator">=</span> attn_scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="27">
</span><span class="code-line line-number" line="28">        <span class="token comment"># 3. 计算注意力权重 (Softmax)</span>
</span><span class="code-line line-number" line="29">        attn_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="30">
</span><span class="code-line line-number" line="31">        <span class="token comment"># 4. 加权求和 (权重 * V)</span>
</span><span class="code-line line-number" line="32">        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn_probs<span class="token punctuation">,</span> V<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="33">        <span class="token keyword">return</span> output
</span><span class="code-line line-number" line="34">
</span><span class="code-line line-number" line="35">    <span class="token keyword">def</span> <span class="token function">split_heads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="36">        <span class="token comment"># 将输入 x 的形状从 (batch_size, seq_length, d_model)</span>
</span><span class="code-line line-number" line="37">        <span class="token comment"># 变换为 (batch_size, num_heads, seq_length, d_k)</span>
</span><span class="code-line line-number" line="38">        batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> d_model <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="39">        <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="40">
</span><span class="code-line line-number" line="41">    <span class="token keyword">def</span> <span class="token function">combine_heads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="42">        <span class="token comment"># 将输入 x 的形状从 (batch_size, num_heads, seq_length, d_k)</span>
</span><span class="code-line line-number" line="43">        <span class="token comment"># 变回 (batch_size, seq_length, d_model)</span>
</span><span class="code-line line-number" line="44">        batch_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> d_k <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="45">        <span class="token keyword">return</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="46">
</span><span class="code-line line-number" line="47">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="48">        <span class="token comment"># 1. 对 Q, K, V 进行线性变换</span>
</span><span class="code-line line-number" line="49">        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>split_heads<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_q<span class="token punctuation">(</span>Q<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="50">        K <span class="token operator">=</span> self<span class="token punctuation">.</span>split_heads<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_k<span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="51">        V <span class="token operator">=</span> self<span class="token punctuation">.</span>split_heads<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_v<span class="token punctuation">(</span>V<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="52">
</span><span class="code-line line-number" line="53">        <span class="token comment"># 2. 计算缩放点积注意力</span>
</span><span class="code-line line-number" line="54">        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="55">
</span><span class="code-line line-number" line="56">        <span class="token comment"># 3. 合并多头输出并进行最终的线性变换</span>
</span><span class="code-line line-number" line="57">        output <span class="token operator">=</span> self<span class="token punctuation">.</span>W_o<span class="token punctuation">(</span>self<span class="token punctuation">.</span>combine_heads<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="58">        <span class="token keyword">return</span> output
</span></code></pre>
<p><strong>（3）前馈神经网络</strong></p>
<p>在每个 Encoder 和 Decoder 层中，多头注意力子层之后都跟着一个<strong>逐位置前馈网络(Position-wise Feed-Forward Network, FFN)</strong> 。如果说注意力层的作用是从整个序列中“动态地聚合”相关信息，那么前馈网络的作用从这些聚合后的信息中提取更高阶的特征。</p>
<p>这个名字的关键在于“逐位置”。它意味着这个前馈网络会独立地作用于序列中的每一个词元向量。换句话说，对于一个长度为 <code>seq_len</code> 的序列，这个 FFN 实际上会被调用 <code>seq_len</code> 次，每次处理一个词元。重要的是，所有位置共享的是同一组网络权重。这种设计既保持了对每个位置进行独立加工的能力，又大大减少了模型的参数量。这个网络的结构非常简单，由两个线性变换和一个 ReLU 激活函数组成：</p>
<p>$$\mathrm{FFN}(x)=\max\left(0, xW_{1}+b_{1}\right) W_{2}+b_{2}$$</p>
<p>其中，$x$是注意力子层的输出。 $W_1,b_1,W_2,b_2$是可学习的参数。通常，第一个线性层的输出维度 <code>d_ff</code> 会远大于输入的维度 <code>d_model</code>（例如 <code>d_ff = 4 * d_model</code>），经过 ReLU 激活后再通过第二个线性层映射回 <code>d_model</code> 维度。这种“先扩大再缩小”的模式，也被称为瓶颈结构，被认为有助于模型学习更丰富的特征表示。</p>
<p>在我们的 PyTorch 骨架中，我们可以用以下代码来实现这个模块：</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">class</span> <span class="token class-name">PositionWiseFeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="2">    <span class="token triple-quoted-string string">"""
</span></span><span class="code-line line-number" line="3"><span class="token triple-quoted-string string">    位置前馈网络模块
</span></span><span class="code-line line-number" line="4"><span class="token triple-quoted-string string">    """</span>
</span><span class="code-line line-number" line="5">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="6">        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionWiseFeedForward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="7">        self<span class="token punctuation">.</span>linear1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="8">        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="9">        self<span class="token punctuation">.</span>linear2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="10">        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="11">
</span><span class="code-line line-number" line="12">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="13">        <span class="token comment"># x 形状: (batch_size, seq_len, d_model)</span>
</span><span class="code-line line-number" line="14">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="15">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="16">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="17">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="18">        <span class="token comment"># 最终输出形状: (batch_size, seq_len, d_model)</span>
</span><span class="code-line line-number" line="19">        <span class="token keyword">return</span> x
</span></code></pre>
<p><strong>（4）残差连接与层归一化</strong></p>
<p>在 Transformer 的每个编码器和解码器层中，所有子模块（如多头注意力和前馈网络）都被一个 <code>Add &amp; Norm</code> 操作包裹。这个组合是为了保证 Transformer 能够稳定训练。</p>
<p>这个操作由两个部分组成：</p>
<ul>
<li><strong>残差连接 (Add)</strong>：该操作将子模块的输入 <code>x</code> 直接加到该子模块的输出 <code>Sublayer(x)</code> 上。这一结构解决了深度神经网络中的<strong>梯度消失 (Vanishing Gradients)</strong> 问题。在反向传播时，梯度可以绕过子模块直接向前传播，从而保证了即使网络层数很深，模型也能得到有效的训练。其公式可以表示为：$\text{Output} = x + \text{Sublayer}(x)$。</li>
<li><strong>层归一化 (Norm)</strong>：该操作对单个样本的所有特征进行归一化，使其均值为0，方差为1。这解决了模型训练过程中的<strong>内部协变量偏移 (Internal Covariate Shift)</strong> 问题，使每一层的输入分布保持稳定，从而加速模型收敛并提高训练的稳定性。</li>
</ul>
<p><strong>3.1.2.5 位置编码</strong></p>
<p>我们已经了解，Transformer 的核心是自注意力机制，它通过计算序列中任意两个词元之间的关系来捕捉依赖。然而，这种计算方式有一个固有的问题：它本身不包含任何关于词元顺序或位置的信息。对于自注意力来说，“agent learns” 和 “learns agent” 这两个序列是完全等价的，因为它只关心词元之间的关系，而忽略了它们的排列。为了解决这个问题，Transformer 引入了<strong>位置编码 (Positional Encoding)</strong> 。</p>
<p>位置编码的核心思想是，为输入序列中的每一个词元嵌入向量，都额外加上一个能代表其绝对位置和相对位置信息的“位置向量”。这个位置向量不是通过学习得到的，而是通过一个固定的数学公式直接计算得出。这样一来，即使两个词元（例如，两个都叫 <code>agent</code> 的词元）自身的嵌入是相同的，但由于它们在句子中的位置不同，它们最终输入到 Transformer 模型中的向量就会因为加上了不同的位置编码而变得独一无二。原论文中提出的位置编码使用正弦和余弦函数来生成，其公式如下：</p>
<p>$$PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)，$$</p>
<p>$$PE_{(pos,2i+1)}=\cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$</p>
<p>其中：</p>
<ul>
<li>$pos$ 是词元在序列中的位置（例如，$0$，$1$，$2$，...）</li>
<li>$i$ 是位置向量中的维度索引（从 $0$ 到 $d_{\text{model}}/2$）</li>
<li>$d_{\text{model}}$是词嵌入向量的维度（与我们模型中定义的一致）</li>
</ul>
<p>现在，我们来实现 <code>PositionalEncoding</code> 模块，并完成我们 Transformer 骨架代码的最后一部分。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="2">    <span class="token triple-quoted-string string">"""
</span></span><span class="code-line line-number" line="3"><span class="token triple-quoted-string string">    为输入序列的词嵌入向量添加位置编码。
</span></span><span class="code-line line-number" line="4"><span class="token triple-quoted-string string">    """</span>
</span><span class="code-line line-number" line="5">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> dropout<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> max_len<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="6">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="7">        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="8">
</span><span class="code-line line-number" line="9">        <span class="token comment"># 创建一个足够长的位置编码矩阵</span>
</span><span class="code-line line-number" line="10">        position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="11">        div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">-</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="12">
</span><span class="code-line line-number" line="13">        <span class="token comment"># pe (positional encoding) 的大小为 (max_len, d_model)</span>
</span><span class="code-line line-number" line="14">        pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="15">
</span><span class="code-line line-number" line="16">        <span class="token comment"># 偶数维度使用 sin, 奇数维度使用 cos</span>
</span><span class="code-line line-number" line="17">        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="18">        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="19">
</span><span class="code-line line-number" line="20">        <span class="token comment"># 将 pe 注册为 buffer，这样它就不会被视为模型参数，但会随模型移动（例如 to(device)）</span>
</span><span class="code-line line-number" line="21">        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="22">
</span><span class="code-line line-number" line="23">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
</span><span class="code-line line-number" line="24">        <span class="token comment"># x.size(1) 是当前输入的序列长度</span>
</span><span class="code-line line-number" line="25">        <span class="token comment"># 将位置编码加到输入向量上</span>
</span><span class="code-line line-number" line="26">        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="27">        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</span></code></pre>
<p>本小节主要是为了帮助理解 Transformer 的宏观结构和内部每个模块的运作细节。由于是为了补充智能体学习中大模型的知识体系，也就不再继续往下深入实现。至此，我们已经为理解现代大语言模型打下了坚实的架构基础。在下一节中，我们将探讨 Decoder-Only 架构，看看它是如何基于 Transformer 的思想演变而来。</p>
<h3>3.1.3 Decoder-Only 架构</h3>
<p>前面一节中，我们动手构建了一个完整的Transformer 模型，它能在很多端到端的场景表现出色。但是当任务转换为构建一个与人对话、创作、作为智能体大脑的通用模型时，或许我们并不需要那么复杂的结构。</p>
<p>Transformer的设计哲学是“先理解，再生成”。编码器负责深入理解输入的整个句子，形成一个包含全局信息的上下文记忆，然后解码器基于这份记忆来生成翻译。但 OpenAI 在开发 <strong>GPT (Generative Pre-trained Transformer)</strong> 时，提出了一个更简单的思想<sup>[5]</sup>：<strong>语言的核心任务，不就是预测下一个最有可能出现的词吗？</strong></p>
<p>无论是回答问题、写故事还是生成代码，本质上都是在一个已有的文本序列后面，一个词一个词地添加最合理的内容。基于这个思想，GPT 做了一个大胆的简化：<strong>它完全抛弃了编码器，只保留了解码器部分。</strong> 这就是 <strong>Decoder-Only</strong> 架构的由来。</p>
<p>Decoder-Only 架构的工作模式被称为<strong>自回归 (Autoregressive)</strong> 。这个听起来很专业的术语，其实描述了一个非常简单的过程：</p>
<ol>
<li>给模型一个起始文本（例如 “Datawhale Agent is”）。</li>
<li>模型预测出下一个最有可能的词（例如 “a”）。</li>
<li>模型将自己刚刚生成的词 “a” 添加到输入文本的末尾，形成新的输入（“Datawhale Agent is a”）。</li>
<li>模型基于这个新输入，再次预测下一个词（例如 “powerful”）。</li>
<li>不断重复这个过程，直到生成完整的句子或达到停止条件。</li>
</ol>
<p>模型就像一个在玩“文字接龙”的游戏，它不断地“回顾”自己已经写下的内容，然后思考下一个字该写什么。</p>
<p>你可能会问，解码器是如何保证在预测第 <code>t</code> 个词时，不去“偷看”第 <code>t+1</code> 个词的答案呢？</p>
<p>答案就是<strong>掩码自注意力 (Masked Self-Attention)</strong> 。在 Decoder-Only 架构中，这个机制变得至关重要。它的工作原理非常巧妙：</p>
<p>在自注意力机制计算出注意力分数矩阵（即每个词对其他所有词的关注度得分）之后，但在进行 Softmax 归一化之前，模型会应用一个“掩码”。这个掩码会将所有位于当前位置之后（即目前尚未观测到）的词元对应的分数，替换为一个非常大的负数。当这个带有负无穷分数的矩阵经过 Softmax 函数时，这些位置的概率就会变为 0。这样一来，模型在计算任何一个位置的输出时，都从数学上被阻止了去关注它后面的信息。这种机制保证了模型在预测下一个词时，能且仅能依赖它已经见过的、位于当前位置之前的所有信息，从而确保了预测的公平性和逻辑的连贯性。</p>
<p><strong>Decoder-Only 架构的优势</strong></p>
<p>这种看似简单的架构，却带来了巨大的成功，其优势在于：</p>
<ul>
<li><strong>训练目标统一</strong>：模型的唯一任务就是“预测下一个词”，这个简单的目标非常适合在海量的无标注文本数据上进行预训练。</li>
<li><strong>结构简单，易于扩展</strong>：更少的组件意味着更容易进行规模化扩展。今天的 GPT-4、Llama 等拥有数千亿甚至万亿参数的巨型模型，都是基于这种简洁的架构。</li>
<li><strong>天然适合生成任务</strong>：其自回归的工作模式与所有生成式任务（对话、写作、代码生成等）完美契合，这也是它能成为构建通用智能体基础的核心原因。</li>
</ul>
<p>总而言之，从 Transformer 的解码器演变而来的 Decoder-Only 架构，通过“预测下一个词”这一简单的范式，开启了我们今天所处的大语言模型时代。</p>
<h2>3.2 与大语言模型交互</h2>
<h3>3.2.1 提示工程</h3>
<p>如果我们把大语言模型比作一个能力极强的“大脑”，那么<strong>提示 (Prompt)</strong> 就是我们与这个“大脑”沟通的语言。提示工程，就是研究如何设计出精准的提示，从而引导模型产生我们期望输出的回复。对于构建智能体而言，一个精心设计的提示能让智能体之间协作分工变得高效。</p>
<p><strong>（1）模型采样参数</strong></p>
<p>在使用大模型时，你会经常看到类似<code>Temperature</code>这类的可配置参数，其本质是通过调整模型对 “概率分布” 的采样策略，让输出匹配具体场景需求，配置合适的参数可以提升Agent在特定场景的性能。</p>
<p>传统的概率分布使由 Softmax 公式计算得到的：$p_i = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}$，采样参数的本质就是在此基础上，根据不同策略“重新调整”或“截断”分布，从而改变大模型输出的下一个token。</p>
<p><code>Temperature</code>：温度是控制模型输出 “随机性” 与 “确定性” 的关键参数。其原理是引入温度系数$T\gt0$,将 Softmax 改写为$p_i^{(T)} = \frac{e^{z_i / T}}{\sum_{j=1}^k e^{z_j / T}}$。</p>
<p>当T变小时，分布“更加陡峭”，高概率项权重进一步放大，生成更“保守”且重复率更高的文本。当T变大时，分布“更加平坦”，低概率项权重提升，生成更“多样”但可能出现不连贯的内容。</p>
<ul>
<li>
<p>低温度（0 $\leqslant$ Temperature $\lt$ 0.3）时输出更 “精准、确定”。适用场景： 事实性任务：如问答、数据计算、代码生成； 严谨性场景：法律条文解读、技术文档撰写、学术概念解释等场景。</p>
</li>
<li>
<p>中温度（0.3 $\leqslant$ Temperature $\lt$ 0.7）：输出 “平衡、自然”。适用场景： 日常对话：如客服交互、聊天机器人； 常规创作：如邮件撰写、产品文案、简单故事创作。</p>
</li>
<li>
<p>高温度（0.7 $\leqslant$ Temperature $\lt$ 2）：输出 “创新、发散”。适用场景： 创意性任务：如诗歌创作、科幻故事构思、广告 slogan brainstorm、艺术灵感启发； 发散性思考。</p>
</li>
</ul>
<p><code>Top-k </code>：其原理是将所有 token 按概率从高到低排序，取排名前 k 个的 token 组成 “候选集”，随后对筛选出的 k 个 token 的概率进行 “归一化”： $ \hat{p}<em>i = \frac{p_i}{\sum</em>{j \in \text{候选集}} p_j}$</p>
<ul>
<li>与温度采样的区别与联系：温度采样通过温度 T 调整所有 token 的概率分布（平滑或陡峭），不改变候选 token 的数量（仍考虑全部 N 个）。Top-k 采样通过 k 值限制候选 token 的数量（只保留前 k 个高概率 token），再从其中采样。当k=1时输出完全确定，退化为 “贪心采样”。</li>
</ul>
<p><code>Top-p </code>：其原理是将所有 token 按概率从高到低排序，从排序后的第一个 token 开始，逐步累加概率，直到累积和首次达到或超过阈值 p： $\sum_{i \in S} p_{(i)} \geq p$，此时累加过程中包含的所有 token 组成 “核集合”，最后对核集合进行归一化。</p>
<ul>
<li>与Top-k的区别与联系：相对于固定截断大小的 Top-k，Top-p 能动态适应不同分布的“长尾”特性，对概率分布不均匀的极端情况的适应性更好。</li>
</ul>
<p>在文本生成中，当同时设置 Top-p、Top-k 和温度系数时，这些参数会按照分层过滤的方式协同工作，其优先级顺序为：温度调整→Top-k→Top-p。温度调整整体分布的陡峭程度，Top-k 会先保留概率最高的 k 个候选，然后 Top-p 会从 Top-k 的结果中选取累积概率≥p 的最小集合作为最终的候选集。不过，通常 Top-k 和 Top-p 二选一即可，若同时设置，实际候选集为两者的交集。
需要注意的是，如果将温度设置为 0，则 Top-k 和 Top-p 将变得无关紧要，因为最有可能的 Token 将成为下一个预测的 Token；如果将 Top-k 设置为 1，温度和 Top-p 也将变得无关紧要，因为只有一个 Token 通过 Top-k 标准，它将是下一个预测的 Token。</p>
<p><strong>（2）零样本、单样本与少样本提示</strong></p>
<p>根据我们给模型提供示例（Exemplar）的数量，提示可以分为三种类型。为了更好地理解它们，让我们以一个情感分类任务为例，目标是让模型判断一段文本的情感色彩（如正面、负面或中性）。</p>
<p><strong>零样本提示 (Zero-shot Prompting)</strong> 这指的是我们不给模型任何示例，直接让它根据指令完成任务。这得益于模型在海量数据上预训练后获得的强大泛化能力。</p>
<p>案例： 我们直接向模型下达指令，要求它完成情感分类任务。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1">文本<span class="token punctuation">:</span>Datawhale的AI Agent课程非常棒！
</span><span class="code-line line-number" line="2">情感<span class="token punctuation">:</span>正面
</span></code></pre>
<p><strong>单样本提示 (One-shot Prompting)</strong> 我们给模型提供一个完整的示例，向它展示任务的格式和期望的输出风格。</p>
<p>我们给模型提供一个完整的示例，向它展示任务的格式和期望的输出风格。</p>
<p>案例： 我们先给模型一个完整的“问题-答案”对作为示范，然后提出我们的新问题。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1">文本<span class="token punctuation">:</span>这家餐厅的服务太慢了。
</span><span class="code-line line-number" line="2">情感<span class="token punctuation">:</span>负面
</span><span class="code-line line-number" line="3">
</span><span class="code-line line-number" line="4">文本<span class="token punctuation">:</span>Datawhale的AI Agent课程非常棒！
</span><span class="code-line line-number" line="5">情感<span class="token punctuation">:</span>
</span></code></pre>
<p>模型会模仿给出的示例格式，为第二段文本补全“正面”。</p>
<p><strong>少样本提示 (Few-shot Prompting)</strong> 我们提供多个示例，这能让模型更准确地理解任务的细节、边界和细微差别，从而获得更好的性能。</p>
<p>案例： 我们提供涵盖了不同情况的多个示例，让模型对任务有更全面的理解。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1">文本<span class="token punctuation">:</span>这家餐厅的服务太慢了。
</span><span class="code-line line-number" line="2">情感<span class="token punctuation">:</span>负面
</span><span class="code-line line-number" line="3">
</span><span class="code-line line-number" line="4">文本<span class="token punctuation">:</span>这部电影的情节很平淡。
</span><span class="code-line line-number" line="5">情感<span class="token punctuation">:</span>中性
</span><span class="code-line line-number" line="6">
</span><span class="code-line line-number" line="7">文本<span class="token punctuation">:</span>Datawhale的AI Agent课程非常棒！
</span><span class="code-line line-number" line="8">情感<span class="token punctuation">:</span>
</span></code></pre>
<p>模型会综合所有示例，更准确地将最后一句的情感分类为“正面”。</p>
<p><strong>（3）指令调优的影响</strong></p>
<p>早期的 GPT 模型（如 GPT-3）主要是“文本补全”模型，它们擅长根据前面的文本续写，但不一定能很好地理解并执行人类的指令。</p>
<p><strong>指令调优 (Instruction Tuning)</strong> 是一种微调技术，它使用大量“指令-回答”格式的数据对预训练模型进行进一步的训练。经过指令调优后，模型能更好地理解并遵循用户的指令。我们今天日常工作学习中使用的所有模型（如 <code>ChatGPT</code>, <code>DeepSeek</code>, <code>Qwen</code>）都是其模型家族中经过指令调优过的模型。</p>
<ul>
<li><strong>对“文本补全”模型的提示(你需要用少样本提示“教会”模型做什么)：</strong></li>
</ul>
<pre class="language-plain"><code class="language-Plain code-highlight"><span class="code-line line-number" line="1">这是一段将英文翻译成中文的程序。
</span><span class="code-line line-number" line="2">英文:Hello
</span><span class="code-line line-number" line="3">中文:你好
</span><span class="code-line line-number" line="4">英文:How are you?
</span><span class="code-line line-number" line="5">中文:
</span></code></pre>
<ul>
<li><strong>对“指令调优”模型的提示(你可以直接下达指令)：</strong></li>
</ul>
<pre class="language-plain"><code class="language-Plain code-highlight"><span class="code-line line-number" line="1">请将下面的英文翻译成中文:
</span><span class="code-line line-number" line="2">How are you?
</span></code></pre>
<p>指令调优的出现，极大地简化了我们与模型交互的方式，使得直接、清晰的自然语言指令成为可能。</p>
<p><strong>（4）基础提示技巧</strong></p>
<p><strong>角色扮演 (Role-playing)</strong> 通过赋予模型一个特定的角色，我们可以引导它的回答风格、语气和知识范围，使其输出更符合特定场景的需求。</p>
<pre class="language-plain"><code class="language-Plain code-highlight"><span class="code-line line-number" line="1"># 案例
</span><span class="code-line line-number" line="2">你现在是一位资深的Python编程专家。请解释一下Python中的GIL（全局解释器锁）是什么，要让一个初学者也能听懂。
</span></code></pre>
<p><strong>上下文示例 (In-context Example)</strong> 这与少样本提示的思想一致，通过在提示中提供清晰的输入输出示例，来“教会”模型如何处理我们的请求，尤其是在处理复杂格式或特定风格的任务时非常有效。</p>
<pre class="language-plain"><code class="language-Plain code-highlight"><span class="code-line line-number" line="1"># 案例
</span><span class="code-line line-number" line="2">我需要你从产品评论中提取产品名称和用户情感。请严格按照下面的JSON格式输出。
</span><span class="code-line line-number" line="3">
</span><span class="code-line line-number" line="4">评论:这款“星尘”笔记本电脑的屏幕显示效果惊人，但我不太喜欢它的键盘手感。
</span><span class="code-line line-number" line="5">输出:{"product_name": "星尘笔记本电脑", "sentiment": "混合"}
</span><span class="code-line line-number" line="6">
</span><span class="code-line line-number" line="7">评论:我刚买的“声动”耳机音质很棒，续航也超出了我的预期！
</span><span class="code-line line-number" line="8">输出:
</span></code></pre>
<p><strong>（5）思维链</strong></p>
<p>对于需要逻辑推理、计算或多步骤思考的复杂问题，直接让模型给出答案往往容易出错。<strong>思维链 (Chain-of-Thought, CoT)</strong> 是一种强大的提示技巧，它通过引导模型“一步一步地思考”，提升了模型在复杂任务上的推理能力。</p>
<p>实现 CoT 的关键，是在提示中加入一句简单的引导语，如“请逐步思考”或“Let's think step by step”。</p>
<pre class="language-plain"><code class="language-Plain code-highlight"><span class="code-line line-number" line="1"># 思维链提示
</span><span class="code-line line-number" line="2">一个篮球队在一个赛季的80场比赛中赢了60%。在接下来的赛季中，他们打了15场比赛，赢了12场。两个赛季的总胜率是多少？
</span><span class="code-line line-number" line="3">请一步一步地思考并解答。
</span><span class="code-line line-number" line="4">
</span><span class="code-line line-number" line="5">&gt;&gt;&gt;
</span><span class="code-line line-number" line="6">（模型可能会输出）
</span><span class="code-line line-number" line="7">好的，我们来一步步计算。
</span><span class="code-line line-number" line="8">第一步:计算第一个赛季赢得的比赛数。
</span><span class="code-line line-number" line="9">80场 * 60% = 48场。
</span><span class="code-line line-number" line="10">第二步:计算两个赛季的总比赛数和总胜利数。
</span><span class="code-line line-number" line="11">总比赛数 = 80 + 15 = 95场。
</span><span class="code-line line-number" line="12">总胜利数 = 48 + 12 = 60场。
</span><span class="code-line line-number" line="13">第三步:计算总胜率。
</span><span class="code-line line-number" line="14">总胜率 = (总胜利数 / 总比赛数) * 100% = (60 / 95) * 100% ≈ 63.16%。
</span><span class="code-line line-number" line="15">所以，两个赛季的总胜率约为63.16%。
</span></code></pre>
<p>通过显式地展示其推理过程，模型不仅更容易得出正确的答案，也让它的回答变得更可信、更易于我们检查和纠正。</p>
<h3>3.2.2 文本分词</h3>
<p>我们知道，计算机本质上只能理解数字。因此，在将自然语言文本喂给大语言模型之前，必须先将其转换成模型能够处理的数字格式。这个将文本序列转换为数字序列的过程，就叫做<strong>分词 (Tokenization)</strong> 。<strong>分词器 (Tokenizer)</strong> 的作用，就是定义一套规则，将原始文本切分成一个个最小的单元，我们称之为<strong>词元 (Token)</strong> 。</p>
<p><strong>3.2.2.1 为何需要分词</strong></p>
<p>早期的自然语言处理任务可能会采用简单的分词策略：</p>
<ul>
<li><strong>按词分词 (Word-based)</strong> ：直接用空格或标点符号将句子切分成单词。这种方法很直观，但会面临“词表爆炸”的问题。一个语言的词汇量是巨大的，如果每个词都作为一个独立的词元，词表会变得难以管理。更糟糕的是，模型将无法处理任何未在词表中出现过的词，例如 “DatawhaleAgent”。</li>
<li><strong>按字符分词 (Character-based)</strong> ：将文本切分成单个字符。这种方法词表很小（例如英文字母、数字和标点），不存在 OOV 问题。但它的缺点是，单个字符大多不具备独立的语义，模型需要花费更多的精力去学习如何将字符组合成有意义的词，导致学习效率低下。</li>
</ul>
<p>为了兼顾词表大小和语义表达，现代大语言模型普遍采用<strong>子词分词 (Subword Tokenization)</strong> 算法。它的核心思想是：将常见的词（如 "agent"）保留为完整的词元，同时将不常见的词（如 "Tokenization"）拆分成多个有意义的子词片段（如 "Token" 和 "ization"）。这样既控制了词表的大小，又能让模型通过组合子词来理解和生成新词。</p>
<p><strong>3.2.2.2 字节对编码算法解析</strong></p>
<p>字节对编码 (Byte-Pair Encoding, BPE) 是最主流的子词分词算法之一<sup>[6]</sup>，GPT系列模型就采用了这种算法。其核心思想非常简洁，可以理解为一个“贪心”的合并过程：</p>
<ol>
<li><strong>初始化</strong>：将词表初始化为所有在语料库中出现过的基本字符。</li>
<li><strong>迭代合并</strong>：在语料库上，统计所有相邻词元对的出现频率，找到频率最高的一对，将它们合并成一个新的词元，并加入词表。</li>
<li><strong>重复</strong>：重复第 2 步，直到词表大小达到预设的阈值。</li>
</ol>
<p><strong>案例演示：</strong> 假设我们的迷你语料库是 <code>{"hug": 1, "pug": 1, "pun": 1, "bun": 1}</code>，并且我们想构建一个大小为 10 的词表。BPE 的训练过程可以用下表3.1来表示：</p>
<div align="center">
  <p>表 3.1  BPE 算法合并过程示例</p>
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-5.png" alt="图片描述" width="90%">
</div>
<p>训练结束后，词表大小达到 10，我们就得到了新的分词规则。现在，对于一个未见过的词 "bug"，分词器会先查找 "bug" 是否在词表中，发现不在；然后查找 "bu"，发现不在；最后查找 "b" 和 "ug"，发现都在，于是将其切分为 <code>['b', 'ug']</code>。</p>
<p>下面我们用一段简单的 Python 代码来模拟上述过程：</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">import</span> re<span class="token punctuation">,</span> collections
</span><span class="code-line line-number" line="2">
</span><span class="code-line line-number" line="3"><span class="token keyword">def</span> <span class="token function">get_stats</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="4">    <span class="token triple-quoted-string string">"""统计词元对频率"""</span>
</span><span class="code-line line-number" line="5">    pairs <span class="token operator">=</span> collections<span class="token punctuation">.</span>defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="6">    <span class="token keyword">for</span> word<span class="token punctuation">,</span> freq <span class="token keyword">in</span> vocab<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="7">        symbols <span class="token operator">=</span> word<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="8">        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>symbols<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="9">            pairs<span class="token punctuation">[</span>symbols<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>symbols<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+=</span> freq
</span><span class="code-line line-number" line="10">    <span class="token keyword">return</span> pairs
</span><span class="code-line line-number" line="11">
</span><span class="code-line line-number" line="12"><span class="token keyword">def</span> <span class="token function">merge_vocab</span><span class="token punctuation">(</span>pair<span class="token punctuation">,</span> v_in<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="13">    <span class="token triple-quoted-string string">"""合并词元对"""</span>
</span><span class="code-line line-number" line="14">    v_out <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</span><span class="code-line line-number" line="15">    bigram <span class="token operator">=</span> re<span class="token punctuation">.</span>escape<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pair<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="16">    p <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">r'(?&lt;!\S)'</span> <span class="token operator">+</span> bigram <span class="token operator">+</span> <span class="token string">r'(?!\S)'</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="17">    <span class="token keyword">for</span> word <span class="token keyword">in</span> v_in<span class="token punctuation">:</span>
</span><span class="code-line line-number" line="18">        w_out <span class="token operator">=</span> p<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pair<span class="token punctuation">)</span><span class="token punctuation">,</span> word<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="19">        v_out<span class="token punctuation">[</span>w_out<span class="token punctuation">]</span> <span class="token operator">=</span> v_in<span class="token punctuation">[</span>word<span class="token punctuation">]</span>
</span><span class="code-line line-number" line="20">    <span class="token keyword">return</span> v_out
</span><span class="code-line line-number" line="21">
</span><span class="code-line line-number" line="22"><span class="token comment"># 准备语料库，每个词末尾加上&lt;/w&gt;表示结束，并切分好字符</span>
</span><span class="code-line line-number" line="23">vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'h u g &lt;/w&gt;'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'p u g &lt;/w&gt;'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'p u n &lt;/w&gt;'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'b u n &lt;/w&gt;'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">}</span>
</span><span class="code-line line-number" line="24">num_merges <span class="token operator">=</span> <span class="token number">4</span> <span class="token comment"># 设置合并次数</span>
</span><span class="code-line line-number" line="25">
</span><span class="code-line line-number" line="26"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_merges<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line line-number" line="27">    pairs <span class="token operator">=</span> get_stats<span class="token punctuation">(</span>vocab<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="28">    <span class="token keyword">if</span> <span class="token keyword">not</span> pairs<span class="token punctuation">:</span>
</span><span class="code-line line-number" line="29">        <span class="token keyword">break</span>
</span><span class="code-line line-number" line="30">    best <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>pairs<span class="token punctuation">,</span> key<span class="token operator">=</span>pairs<span class="token punctuation">.</span>get<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="31">    vocab <span class="token operator">=</span> merge_vocab<span class="token punctuation">(</span>best<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="32">    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"第</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">次合并: </span><span class="token interpolation"><span class="token punctuation">{</span>best<span class="token punctuation">}</span></span><span class="token string"> -&gt; </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>best<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="33">    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"新词表（部分）: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">list</span><span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="34">    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"-"</span> <span class="token operator">*</span> <span class="token number">20</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="35">
</span><span class="code-line line-number" line="36"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
</span><span class="code-line line-number" line="37">第<span class="token number">1</span>次合并<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token string">'u'</span><span class="token punctuation">,</span> <span class="token string">'g'</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> ug
</span><span class="code-line line-number" line="38">新词表（部分）<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'h ug &lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p ug &lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p u n &lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'b u n &lt;/w&gt;'</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="39"><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
</span><span class="code-line line-number" line="40">第<span class="token number">2</span>次合并<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token string">'ug'</span><span class="token punctuation">,</span> <span class="token string">'&lt;/w&gt;'</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> ug<span class="token operator">&lt;</span><span class="token operator">/</span>w<span class="token operator">&gt;</span>
</span><span class="code-line line-number" line="41">新词表（部分）<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'h ug&lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p ug&lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p u n &lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'b u n &lt;/w&gt;'</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="42"><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
</span><span class="code-line line-number" line="43">第<span class="token number">3</span>次合并<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token string">'u'</span><span class="token punctuation">,</span> <span class="token string">'n'</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> un
</span><span class="code-line line-number" line="44">新词表（部分）<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'h ug&lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p ug&lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p un &lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'b un &lt;/w&gt;'</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="45"><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
</span><span class="code-line line-number" line="46">第<span class="token number">4</span>次合并<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token string">'un'</span><span class="token punctuation">,</span> <span class="token string">'&lt;/w&gt;'</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> un<span class="token operator">&lt;</span><span class="token operator">/</span>w<span class="token operator">&gt;</span>
</span><span class="code-line line-number" line="47">新词表（部分）<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'h ug&lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p ug&lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'p un&lt;/w&gt;'</span><span class="token punctuation">,</span> <span class="token string">'b un&lt;/w&gt;'</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="48"><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
</span></code></pre>
<p>这段代码清晰地展示了 BPE 算法如何通过迭代合并最高频的相邻词元对，来逐步构建和扩充词表的过程。</p>
<p>后续的许多算法都是在BPE的基础上进行优化的。其中，Google 开发的 WordPiece 和 SentencePiece 是影响力最大的两种。</p>
<ul>
<li><strong>WordPiece</strong>：Google BERT 模型采用的算法<sup>[7]</sup>。它与 BPE 非常相似，但合并词元的标准不是“最高频率”，而是“能最大化提升语料库的语言模型概率”。简单来说，它会优先合并那些能让整个语料库的“通顺度”提升最大的词元对。</li>
<li><strong>SentencePiece</strong>：Google 开源的一款分词工具<sup>[8]</sup>，Llama 系列模型采用了此算法。它最大的特点是，将空格也视作一个普通字符（通常用下划线 <code>_</code> 表示）。这使得分词和解码过程完全可逆，且不依赖于特定的语言（例如，它不需要知道中文不使用空格分词）。</li>
</ul>
<p><strong>3.2.2.3 分词器对开发者的意义</strong></p>
<p>理解分词算法的细节并非目的，但作为智能体的开发者，理解分词器的实际影响是重要，这直接关系到智能体的性能、成本和稳定性：</p>
<ul>
<li><strong>上下文窗口限制</strong>：模型的上下文窗口（如 8K, 128K）是以 <strong>Token 数量</strong>计算的，而不是字符数或单词数。同样一段话，在不同语言（如中英文）或不同分词器下，Token 数量可能相差巨大。精确管理输入长度、避免超出上下文限制是构建长时记忆智能体的基础。</li>
<li><strong>API 成本</strong>：大多数模型 API 都是按 Token 数量计费的。了解你的文本会被如何分词，是预估和控制智能体运行成本的关键一步。</li>
<li><strong>模型表现的异常</strong>：有时模型的奇怪表现根源在于分词。例如，模型可能很擅长计算 <code>2 + 2</code>，但对于 <code>2+2</code>（没有空格）就可能出错，因为后者可能被分词器视为一个独立的、不常见的词元。同样，一个词因为首字母大小写不同，也可能被切分成完全不同的 Token 序列，从而影响模型的理解。在设计提示词和解析模型输出时，考虑到这些“陷阱”有助于提升智能体的鲁棒性。</li>
</ul>
<h3>3.2.3 调用开源大语言模型</h3>
<p>在本书的第一章，我们通过 API 来与大语言模型进行交互，以此驱动我们的智能体。这是一种快速、便捷的方式，但并非唯一的方式。对于许多需要处理敏感数据、希望离线运行或想精细控制成本的场景，将大语言模型直接部署在本地就显得至关重要。</p>
<p><strong>Hugging Face Transformers</strong> 是一个强大的开源库，它提供了标准化的接口来加载和使用数以万计的预训练模型。我们将使用它来完成本次实践。</p>
<p><strong>配置环境与选择模型</strong>：为了让大多数读者都能在个人电脑上顺利运行，我们特意选择了一个小规模但功能强大的模型：<code>Qwen/Qwen1.5-0.5B-Chat</code>。这是一个由阿里巴巴达摩院开源的拥有约 5 亿参数的对话模型，它体积小、性能优异，非常适合入门学习和本地部署。</p>
<p>首先，请确保你已经安装了必要的库：</p>
<pre class="language-plain"><code class="language-Plain code-highlight"><span class="code-line line-number" line="1">pip install transformers torch
</span></code></pre>
<p>在 <code>transformers</code> 库中，我们通常使用 <code>AutoModelForCausalLM</code> 和 <code>AutoTokenizer</code> 这两个类来自动加载与模型匹配的权重和分词器。下面这段代码会自动从 Hugging Face Hub 下载所需的模型文件和分词器配置，这可能需要一些时间，具体取决于你的网络速度。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token keyword">import</span> torch
</span><span class="code-line line-number" line="2"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer
</span><span class="code-line line-number" line="3">
</span><span class="code-line line-number" line="4"><span class="token comment"># 指定模型ID</span>
</span><span class="code-line line-number" line="5">model_id <span class="token operator">=</span> <span class="token string">"Qwen/Qwen1.5-0.5B-Chat"</span>
</span><span class="code-line line-number" line="6">
</span><span class="code-line line-number" line="7"><span class="token comment"># 设置设备，优先使用GPU</span>
</span><span class="code-line line-number" line="8">device <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>
</span><span class="code-line line-number" line="9"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Using device: </span><span class="token interpolation"><span class="token punctuation">{</span>device<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="10">
</span><span class="code-line line-number" line="11"><span class="token comment"># 加载分词器</span>
</span><span class="code-line line-number" line="12">tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="13">
</span><span class="code-line line-number" line="14"><span class="token comment"># 加载模型，并将其移动到指定设备</span>
</span><span class="code-line line-number" line="15">model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="16">
</span><span class="code-line line-number" line="17"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"模型和分词器加载完成！"</span><span class="token punctuation">)</span>
</span></code></pre>
<p>我们来创建一个对话提示，Qwen1.5-Chat 模型遵循特定的对话模板。然后，可以将使用上一步加载的 <code>tokenizer</code> 将文本提示转换为模型能够理解的数字 ID（即 Token ID）。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token comment"># 准备对话输入</span>
</span><span class="code-line line-number" line="2">messages <span class="token operator">=</span> <span class="token punctuation">[</span>
</span><span class="code-line line-number" line="3">    <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"You are a helpful assistant."</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
</span><span class="code-line line-number" line="4">    <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"你好，请介绍你自己。"</span><span class="token punctuation">}</span>
</span><span class="code-line line-number" line="5"><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="6">
</span><span class="code-line line-number" line="7"><span class="token comment"># 使用分词器的模板格式化输入</span>
</span><span class="code-line line-number" line="8">text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
</span><span class="code-line line-number" line="9">    messages<span class="token punctuation">,</span>
</span><span class="code-line line-number" line="10">    tokenize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
</span><span class="code-line line-number" line="11">    add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span>
</span><span class="code-line line-number" line="12"><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="13">
</span><span class="code-line line-number" line="14"><span class="token comment"># 编码输入文本</span>
</span><span class="code-line line-number" line="15">model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>text<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="16">
</span><span class="code-line line-number" line="17"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"编码后的输入文本:"</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="18"><span class="token keyword">print</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="19">
</span><span class="code-line line-number" line="20"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
</span><span class="code-line line-number" line="21"><span class="token punctuation">{</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">151644</span><span class="token punctuation">,</span> <span class="token number">8948</span><span class="token punctuation">,</span> <span class="token number">198</span><span class="token punctuation">,</span> <span class="token number">2610</span><span class="token punctuation">,</span> <span class="token number">525</span><span class="token punctuation">,</span> <span class="token number">264</span><span class="token punctuation">,</span>  <span class="token number">10950</span><span class="token punctuation">,</span> <span class="token number">17847</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span><span class="token number">151645</span><span class="token punctuation">,</span> <span class="token number">198</span><span class="token punctuation">,</span> <span class="token number">151644</span><span class="token punctuation">,</span> <span class="token number">872</span><span class="token punctuation">,</span> <span class="token number">198</span><span class="token punctuation">,</span> <span class="token number">108386</span><span class="token punctuation">,</span> <span class="token number">37945</span><span class="token punctuation">,</span> <span class="token number">100157</span><span class="token punctuation">,</span> <span class="token number">107828</span><span class="token punctuation">,</span><span class="token number">1773</span><span class="token punctuation">,</span> <span class="token number">151645</span><span class="token punctuation">,</span> <span class="token number">198</span><span class="token punctuation">,</span> <span class="token number">151644</span><span class="token punctuation">,</span> <span class="token number">77091</span><span class="token punctuation">,</span> <span class="token number">198</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
</span><span class="code-line line-number" line="22">       device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</span></code></pre>
<p>现在可以调用模型的 <code>generate()</code> 方法来生成回答了。模型会输出一系列 Token ID，这代表了它的回答。</p>
<p>最后，我们需要使用分词器的 <code>decode()</code> 方法，将这些数字 ID 翻译回人类可以阅读的文本。</p>
<pre class="language-python"><code class="language-Python code-highlight"><span class="code-line line-number" line="1"><span class="token comment"># 使用模型生成回答</span>
</span><span class="code-line line-number" line="2"><span class="token comment"># max_new_tokens 控制了模型最多能生成多少个新的Token</span>
</span><span class="code-line line-number" line="3">generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
</span><span class="code-line line-number" line="4">    model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span>
</span><span class="code-line line-number" line="5">    max_new_tokens<span class="token operator">=</span><span class="token number">512</span>
</span><span class="code-line line-number" line="6"><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="7">
</span><span class="code-line line-number" line="8"><span class="token comment"># 将生成的 Token ID 截取掉输入部分</span>
</span><span class="code-line line-number" line="9"><span class="token comment"># 这样我们只解码模型新生成的部分</span>
</span><span class="code-line line-number" line="10">generated_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
</span><span class="code-line line-number" line="11">    output_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> input_ids<span class="token punctuation">,</span> output_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> generated_ids<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="12"><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="13">
</span><span class="code-line line-number" line="14"><span class="token comment"># 解码生成的 Token ID</span>
</span><span class="code-line line-number" line="15">response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generated_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</span><span class="code-line line-number" line="16">
</span><span class="code-line line-number" line="17"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n模型的回答:"</span><span class="token punctuation">)</span>
</span><span class="code-line line-number" line="18"><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</span><span class="code-line line-number" line="19">
</span><span class="code-line line-number" line="20"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
</span><span class="code-line line-number" line="21">我叫通义千问，是由阿里云研发的预训练语言模型，可以回答问题、创作文字，还能表达观点、撰写代码。我主要的功能是在多个领域提
</span><span class="code-line line-number" line="22">供帮助，包括但不限于<span class="token punctuation">:</span>语言理解、文本生成、机器翻译、问答系统等。有什么我可以帮到你的吗？
</span></code></pre>
<p>当你运行完所有代码后，你将会在本地电脑上看到模型生成的关于Qwen模型的介绍。恭喜你，你已经成功地在本地部署并运行了一个开源大语言模型！</p>
<h3>3.2.4 模型的选择</h3>
<p>在上一节中，我们成功地在本地运行了一个小型的开源语言模型。这自然引出了一个对于智能体开发者而言至关重要的问题：在当前数百个模型百花齐放的背景下，我们应当如何为特定的任务选择最合适的模型？</p>
<p>选择语言模型并非简单地追求“最大、最强”，而是一个在性能、成本、速度和部署方式之间进行权衡的决策过程。本节将首先梳理模型选型的几个关键考量因素，然后对当前主流的闭源与开源模型进行梳理。</p>
<p>由于大语言模型技术正处于高速发展阶段，新模型、新版本层出不穷，迭代速度极快。本节在撰写时力求提供当前主流模型的概览和选型考量，但请读者注意，文中所提及的具体模型版本和性能数据可能随时间推移而发生变化，且只列举了部分工作并不完整。我们更侧重于介绍其核心技术特点、发展趋势以及在智能体开发中的通用选型原则。</p>
<p><strong>3.2.4.1 模型选型的关键考量</strong></p>
<p>在为您的智能体选择大语言模型时，可以从以下几个维度进行综合评估：</p>
<ul>
<li><strong>性能与能力</strong>：这是最核心的考量。不同的模型擅长的任务不同，有的长于逻辑推理和代码生成，有的则在创意写作或多语言翻译上更胜一筹。您可以参考一些公开的基准测试排行榜（如 LMSys Chatbot Arena Leaderboard）来评估模型的综合能力。</li>
<li><strong>成本</strong>：对于闭源模型，成本主要体现在 API 调用费用，通常按 Token 数量计费。对于开源模型，成本则体现在本地部署所需的硬件（GPU、内存）和运维上。需要根据应用的预期使用量和预算做出选择。</li>
<li><strong>速度（延迟）</strong>：对于需要实时交互的智能体（如客服、游戏 NPC），模型的响应速度至关重要。一些轻量级或经过优化的模型（如 GPT-3.5 Turbo, Claude 3.5 Sonnet）在延迟上表现更优。</li>
<li><strong>上下文窗口</strong>：模型能一次性处理的 Token 数量上限。对于需要理解长文档、分析代码库或维持长期对话记忆的智能体，选择一个拥有较大上下文窗口（如 128K Token 或更高）的模型是必要的。</li>
<li><strong>部署方式</strong>：使用 API 的方式最简单便捷，但数据需要发送给第三方，且受限于服务商的条款。本地部署则能确保数据隐私和最高程度的自主可控，但对技术和硬件要求更高。</li>
<li><strong>生态与工具链</strong>：一个模型的流行程度也决定了其周边生态的成熟度。主流模型通常拥有更丰富的社区支持、教程、预训练模型、微调工具和兼容的开发框架（如 LangChain, LlamaIndex, Hugging Face Transformers），这能极大地加速开发进程，降低开发难度。选择一个拥有活跃社区和完善工具链的模型，可以在遇到问题时更容易找到解决方案和资源。</li>
<li><strong>可微调性与定制化</strong>：对于需要处理特定领域数据或执行特定任务的智能体，模型的微调能力至关重要。一些模型提供了便捷的微调接口和工具，允许开发者使用自己的数据集对模型进行定制化训练，从而显著提升模型在特定场景下的性能和准确性。开源模型在这方面通常提供更大的灵活性。</li>
<li><strong>安全性与伦理</strong>：随着大语言模型的广泛应用，其潜在的安全风险和伦理问题也日益凸显。选择模型时，需要考虑其在偏见、毒性、幻觉等方面的表现，以及服务商或开源社区在模型安全和负责任AI方面的投入。对于面向公众或涉及敏感信息的应用，模型的安全性和伦理合规性是不可忽视的考量。</li>
</ul>
<p><strong>3.2.4.2 闭源模型概览</strong></p>
<p>闭源模型通常代表了当前 AI 技术的最前沿，并提供稳定、易用的 API 服务，是构建高性能智能体的首选。</p>
<ol>
<li><strong>OpenAI GPT 系列</strong>：从开启大模型时代的 GPT-3，到引入 RLHF（人类反馈强化学习）、实现与人类意图对齐的 ChatGPT，再到开启多模态时代的 GPT-4，OpenAI 持续引领行业发展。最新的 GPT-5 更是将多模态能力和通用智能水平提升到新的高度，能够无缝处理文本、音频和图像输入，并生成相应的输出，其响应速度和自然度也大幅提升，尤其在实时语音对话方面表现出色。</li>
<li><strong>Google Gemini 系列</strong>：Google DeepMind 推出的 Gemini 系列模型是原生多模态的代表，其核心特点是能统一处理文本、代码、音视频和图像等多种模态的数据，并以其超长的上下文窗口在海量信息处理上具备优势。Gemini Ultra 是其最强大的模型，适用于高度复杂的任务；Gemini Pro 适用于广泛的任务，提供高性能和效率；Gemini Nano 则针对设备端部署进行了优化。最新的 Gemini 2.5 系列模型，如 Gemini 2.5 Pro 和 Gemini 2.5 Flash，进一步提升了推理能力和上下文窗口，特别是 Gemini 2.5 Flash 以其更快的推理速度和成本效益，适用于需要快速响应的场景。</li>
<li><strong>Anthropic Claude 系列</strong>：Anthropic 是一家专注于 AI 安全和负责任 AI 的公司，其 Claude 系列模型从设计之初就将 AI 安全放在首位，以其在处理长文档、减少有害输出、遵循指令方面的可靠性而闻名，深受企业级应用青睐。Claude 3 系列包括 Claude 3 Opus（最智能、性能最强）、Claude 3 Sonnet（性能与速度兼顾的平衡之选）和 Claude 3 Haiku（最快、最紧凑的模型，适用于近乎实时的交互）。最新的 Claude 4 系列模型，如 Claude 4 Opus，在通用智能、复杂推理和代码生成方面取得了显著进展，进一步提升了处理长上下文和多模态任务的能力。</li>
<li><strong>国内主流模型</strong>：中国在大语言模型领域涌现出众多具有竞争力的闭源模型，以百度文心一言(ERNIE Bot)、腾讯混元(Hunyuan)、华为盘古(Pangu-α)、科大讯飞星火(SparkDesk)和月之暗面(Moonshot AI)等为代表的国产模型，在中文处理上具备天然优势，并深度赋能本土产业。</li>
</ol>
<p><strong>3.2.4.3 开源模型概览</strong></p>
<p>开源模型为开发者提供了最高程度的灵活性、透明度和自主性，催生了繁荣的社区生态。它们允许开发者在本地部署、进行定制化微调，并拥有完整的模型控制权。</p>
<ul>
<li><strong>Meta Llama 系列</strong>：Meta 推出的 Llama 系列是开源大语言模型的重要里程碑。该系列凭借出色的综合性能、开放的许可协议和强大的社区支持，成为许多衍生项目和研究的基座。Llama 4 系列于2025年4月发布，是Meta首批采用混合专家（MoE）架构的模型，该架构通过仅激活处理特定任务所需的模型部分来显著提升计算效率。该系列包含三款定位分明的模型：LLama 4 Scout支持1000万token的上下文窗口专为长文档分析和移动端部署设计。Llama 4 Maverick专注于多模态能力，在编码、复杂推理及多语言支持方面表现卓越。Llama 4 Behemoth多项STEM基准测试中表现超越竞争对手。是Meta目前最强大的模型</li>
<li><strong>Mistral AI 系列</strong>：来自法国的 Mistral AI 以其“小尺寸、高性能”的模型设计而闻名。其最新模型 Mistral Medium 3.1 于2025年8月发布，在代码生成、STEM推理和跨领域问答等任务上准确率与响应速度均有显著提升，基准测试表现优于Claude Sonnet 3.7与Llama 4 Maverick等同级模型。它具备原生多模态能力，可同时处理图像与文字混合输入，并内置“语调适配层”，帮助企业更轻松实现符合品牌调性的输出。</li>
<li><strong>国内开源力量</strong>：国内厂商和科研机构也在积极拥抱开源，例如阿里巴巴的<strong>通义千问 (Qwen)</strong> 系列和清华大学与智谱 AI 合作的 <strong>ChatGLM</strong> 系列，它们提供了强大的中文能力，并围绕自身构建了活跃的社区。</li>
</ul>
<p>对于智能体开发者而言，闭源模型提供了“开箱即用”的便捷，而开源模型则赋予了我们“随心所欲”的定制自由。理解这两大阵营的特点和代表模型，是为我们的智能体项目做出明智技术选型的第一步。</p>
<h2>3.3 大语言模型的缩放法则与局限性</h2>
<p>大语言模型（LLMs）在近年来取得了令人瞩目的进展，其能力边界不断拓展，应用场景日益丰富。然而，这些成就的背后，离不开对模型规模、数据量和计算资源之间关系的深刻理解，即<strong>缩放法则（Scaling Laws）</strong>。同时，作为新兴技术，LLMs也面临着诸多挑战和局限性。本节将深入探讨这些核心概念，旨在帮助读者全面理解LLMs的能力边界，从而在构建智能体时扬长避短。</p>
<h3>3.3.1 缩放法则</h3>
<p><strong>缩放法则（Scaling Laws）</strong>是近年来大语言模型领域最重要的发现之一。它揭示了模型性能与模型参数量、训练数据量以及计算资源之间存在着可预测的幂律关系。这一发现为大语言模型的持续发展提供了理论指导，阐明了增加资源投入能够系统性提升模型性能的底层逻辑。</p>
<p>研究发现，在对数-对数坐标系下，模型的性能（通常用损失 Loss 来衡量）与参数量、数据量和计算量这三个因素都呈现出平滑的幂律关系<sup>[9]</sup>。简单来说，只要我们持续、按比例地增加这三个要素，模型的性能就会可预测地、平滑地提升，而不会出现明显的瓶颈。这一发现为大模型的设计和训练提供了清晰的指导：在资源允许的范围内，尽可能地扩大模型规模和训练数据量。</p>
<p>早期的研究更侧重于增加模型参数量，但 DeepMind 在 2022 年提出的“Chinchilla 定律”对此进行了重要修正<sup>[10]</sup>。该定律指出，在给定的计算预算下，为了达到最优性能，<strong>模型参数量和训练数据量之间存在一个最优配比</strong>。具体来说，最优的模型应该比之前普遍认为的要小，但需要用多得多的数据进行训练。例如，一个 700 亿参数的 Chinchilla 模型，由于使用了比 GPT-3（1750 亿参数）多 4 倍的数据进行训练，其性能反而超越了后者。这一发现纠正了“越大越好”的片面认知，强调了数据效率的重要性，并指导了后续许多高效大模型（如 Llama 系列）的设计。</p>
<p>缩放法则最令人惊奇的产物是“能力的涌现”。所谓能力涌现，是指当模型规模达到一定阈值后，会突然展现出在小规模模型中完全不存在或表现不佳的全新能力。例如，<strong>链式思考 (Chain-of-Thought)</strong> 、<strong>指令遵循 (Instruction Following)</strong> 、多步推理、代码生成等能力，都是在模型参数量达到数百亿甚至千亿级别后才显著出现的。这种现象表明，大语言模型不仅仅是简单地记忆和复述，它们在学习过程中可能形成了某种更深层次的抽象和推理能力。对于智能体开发者而言，能力的涌现意味着选择一个足够大规模的模型，是实现复杂自主决策和规划能力的前提。</p>
<h3>3.3.2 模型幻觉</h3>
<p><strong>模型幻觉（Hallucination）</strong>通常指的是大语言模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件。幻觉的本质是模型在生成过程中，过度自信地“编造”了信息，而非准确地检索或推理。根据其表现形式，幻觉可以被分为多种类型<sup>[11]</sup>，例如：</p>
<ul>
<li><strong>事实性幻觉 (Factual Hallucinations)</strong> ： 模型生成与现实世界事实不符的信息。</li>
<li><strong>忠实性幻觉 (Faithfulness Hallucinations)</strong> ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。</li>
<li><strong>内在幻觉 (Intrinsic Hallucinations)</strong> ： 模型生成的内容与输入信息直接矛盾。</li>
</ul>
<p>幻觉的产生是多方面因素共同作用的结果。首先，训练数据中可能包含错误或矛盾的信息。其次，模型的自回归生成机制决定了它只是在预测下一个最可能的词元，而没有内置的事实核查模块。最后，在面对需要复杂推理的任务时，模型可能会在逻辑链条中出错，从而“编造”出错误的结论。例如：一个旅游规划 Agent，可能会为你推荐一个现实中不存在的景点，或者预订一个航班号错误的机票。</p>
<p>此外，大语言模型还面临着知识时效性不足和训练数据中存在的偏见等挑战。大语言模型的能力来源于其训练数据。这意味着模型所掌握的知识是其训练数据收集时的最新材料。对于在此日期之后发生的事件、新出现的概念或最新的事实，模型将无法感知或正确回答。与此同时训练数据往往包含了人类社会的各种偏见和刻板印象。当模型在这些数据上学习时，它不可避免地会吸收并反映出这些偏见<sup>[12]</sup>。</p>
<p>为了提高大语言模型的可靠性，研究人员和开发者正在积极探索多种检测和缓解幻觉的方法：</p>
<ol>
<li><strong>数据层面</strong>： 通过高质量数据清洗、引入事实性知识以及强化学习与人类反馈 (RLHF) 等方式<sup>[13]</sup>，从源头减少幻觉。</li>
<li><strong>模型层面</strong>： 探索新的模型架构，或让模型能够表达其对生成内容的不确定性。</li>
<li><strong>推理与生成层面</strong>：
<ol>
<li><strong>检索增强生成 (Retrieval-Augmented Generation, RAG)</strong> <sup>[14]</sup>： 这是目前缓解幻觉的有效方法之一。RAG 系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答。</li>
<li><strong>多步推理与验证</strong>： 引导模型进行多步推理，并在每一步进行自我检查或外部验证。</li>
<li><strong>引入外部工具</strong>： 允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算。</li>
</ol>
</li>
</ol>
<p>尽管幻觉问题短期内难以完全消除，但通过上述的策略，可以显著降低其发生频率和影响，提高大语言模型在实际应用中的可靠性和实用性。</p>
<h2>3.4 本章小结</h2>
<p>本章介绍了构建智能体所需的基础知识，重点围绕作为其核心组件的大语言模型 (LLM) 展开。内容从语言模型的早期发展开始，详细讲解了 Transformer 架构，并介绍了与 LLM 进行交互的方法。最后，本章对当前主流的模型生态、发展规律及其固有局限性进行了梳理。</p>
<p><strong>核心知识点回顾：</strong></p>
<ul>
<li><strong>模型演进与核心架构</strong>：本章追溯了从统计语言模型 (N-gram) 到神经网络模型 (RNN, LSTM)，再到奠定现代 LLM 基础的 Transformer 架构。通过“自顶向下”的代码实现，本章拆解了 Transformer 的核心组件，并阐述了自注意力机制在并行计算和捕捉长距离依赖中的关键作用。</li>
<li><strong>与模型的交互方式</strong>：本章介绍了与 LLM 交互的两个核心环节：提示工程 (Prompt Engineering) 和文本分词 (Tokenization)。前者用于指导模型的行为，后者是理解模型输入处理的基础。通过本地部署并运行开源模型的实践，将理论知识应用于实际操作。</li>
<li><strong>模型生态与选型</strong>：本章系统地梳理了为智能体选择模型时需要权衡的关键因素，并概览了以 OpenAI GPT、Google Gemini 为代表的闭源模型和以 Llama、Mistral 为代表的开源模型的特点与定位。</li>
<li><strong>法则与局限</strong>：本章探讨了驱动 LLM 能力提升的缩放法则，阐述了其背后的基本原理。同时，本章也分析了模型存在的如事实幻觉、知识过时等固有局限性，这对于构建可靠、鲁棒的智能体至关重要。</li>
</ul>
<p><strong>从 LLM 基础到构建智能体：</strong></p>
<p>这一章的LLM基础主要是为了帮助大家更好的理解大模型的诞生以及发展过程，其中也蕴含了智能体设计的部分思考。例如，如何设计有效的提示词来引导 Agent 的规划与决策，如何根据任务需求选择合适的模型，以及如何在 Agent 的工作流中加入验证机制以规避模型的幻觉等问题，其解决方案均建立在本章的基础之上。我们现在已经准备好从理论转向实践。在下一章，我们将开始探索智能体经典范式构建，将本章所学的知识应用于实际的智能体设计之中。</p>
<h2>习题</h2>
<ol>
<li>
<p>自然语言处理中，语言模型经历了从统计到神经网络的模型演进。</p>
<ul>
<li>请使用本章提供的迷你语料库（<code>datawhale agent learns</code>, <code>datawhale agent works</code>），计算句子 <code>agent works</code> 在Bigram模型下的概率</li>
<li>N-gram模型的核心假设是马尔可夫假设。请解释这个假设的含义，以及N-gram模型存在哪些根本性局限？</li>
<li>神经网络语言模型（RNN/LSTM）和Transformer分别是如何克服N-gram模型局限的？它们各自的优势是什么？</li>
</ul>
</li>
<li>
<p>Transformer架构<sup>[4]</sup>是现代大语言模型的基础。其中：</p>
<blockquote>
<p><strong>提示</strong>：可以结合本章3.1.2节的代码实现来辅助理解</p>
</blockquote>
<ul>
<li>自注意力机制（Self-Attention）的核心思想是什么？</li>
<li>为什么Transformer能够并行处理序列，而RNN必须串行处理？位置编码（Positional Encoding）在其中起什么作用？</li>
<li>Decoder-Only架构与完整的Encoder-Decoder架构有什么区别？为什么现在主流的大语言模型都采用Decoder-Only架构？</li>
</ul>
</li>
<li>
<p>文本子词分词算法是大语言模型的一项关键技术，负责将文本转换为模型可处理的 token 序列。那为什么不能直接以"字符"或"单词"作为模型的输入单元？BPE（Byte Pair Encoding）算法解决了什么问题？</p>
</li>
<li>
<p>本章3.2.3节介绍了如何本地部署开源大语言模型。请完成以下实践和分析：</p>
<blockquote>
<p><strong>提示</strong>：这是一道动手实践题，建议实际操作</p>
</blockquote>
<ul>
<li>按照本章的指导，在本地部署一个轻量级的开源模型（推荐<a href="https://modelscope.cn/models/Qwen/Qwen3-0.6B">Qwen3-0.6B</a>），并尝试调整采样参数并观察其对输出的影响</li>
<li>选择一个具体任务（如文本分类、信息抽取、代码生成等），设计并对比以下不同的提示策略（如Zero-shot、Few-shot、Chain-of-Thought）对输出结果的效果差异</li>
<li>从性能、成本、可控性、隐私等维度比较闭源模型和开源模型</li>
<li>如果你要构建一个企业级的客服智能体，你会选择哪种类型的模型？需要考虑哪些因素？</li>
</ul>
</li>
<li>
<p>模型幻觉（Hallucination）<sup>[11]</sup>是大语言模型当前存在的关键局限性之一。本章介绍了缓解幻觉的方法（如检索增强生成、多步推理、外部工具调用）</p>
<ul>
<li>请选择其中一种，说明其工作原理和适用场景</li>
<li>调研前沿的研究和论文，是否还有其他的缓解模型幻觉的方法，他们又有哪些改进和优势？</li>
</ul>
</li>
<li>
<p>假设你要设计一个论文辅助阅读智能体，它能够帮助研究人员快速阅读并理解学术论文，包括：总结论文研究的核心内容、回答关于论文的问题、提取关键信息、比较多篇不同论文的观点等。请回答：</p>
<ul>
<li>你会选择哪个模型作为智能体设计时的基座模型？选择时需要考虑哪些因素？</li>
<li>如何设计提示词来引导模型更好地理解学术论文？学术论文通常很长，可能超过模型的上下文窗口限制，你会如何解决这个问题？</li>
<li>学术研究是严谨的，这意味着我们需要确保智能体生成的信息是准确客观忠于原文的。你认为系统中加入哪些设计能够更好的实现这一需求？</li>
</ul>
</li>
</ol>
<h2>参考文献</h2>
<p>[1] Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). A neural probabilistic language model. <em>Journal of Machine Learning Research</em>, 3, 1137-1155.</p>
<p>[2] Elman, J. L. (1990). Finding structure in time. <em>Cognitive Science</em>, 14(2), 179-211.</p>
<p>[3] Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, 9(8), 1735-1780.</p>
<p>[4] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is all you need. In <em>Advances in neural information processing systems</em> (pp. 5998-6008).</p>
<p>[5] Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving language understanding by generative pre-training. OpenAI.</p>
<p>[6] Gage, P. (1994). A new algorithm for data compression. <em>C Users Journal</em>, <em>12</em>(2), 23-38.</p>
<p>[7] Schuster, M., &amp; Nakajima, K. (2012, March). Japanese and korean voice search. In <em>2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em> (pp. 5149-5152). IEEE.</p>
<p>[8] Kudo, T., &amp; Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. <em>arXiv preprint arXiv:1808.06226</em>.</p>
<p>[9] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... &amp; Amodei, D. (2020). Scaling Laws for Neural Language Models. arXiv preprint arXiv:2001.08361.</p>
<p>[10] Hoffmann, J., Borgeaud, E., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, R., ... &amp; Sifre, L. (2022). Training Compute-Optimal Large Language Models. arXiv preprint arXiv:2203.07678.</p>
<p>[11] Ji, Z., Lee, N., Fries, R., Yu, T., &amp; Su, D. (2023). Survey of Hallucination in Large Language Models.</p>
<p>[12] Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Mitchell, M. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? .</p>
<p>[13] Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. <em>arXiv preprint arXiv:1706.03741</em>.</p>
<p>[14] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goswami, N., ... &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. In <em>Advances in neural information processing systems</em> (pp. 9459-9474).</p>