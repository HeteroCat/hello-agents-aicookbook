<!DOCTYPE html>
<html lang="zh-CN" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第三章 大语言模型基础 - Hello Agents</title>

    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <!-- MathJax for Math Formula -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Prism.js for Code Highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8fafc;
            --bg-card: #ffffff;
            --text-primary: #1e293b;
            --text-secondary: #475569;
            --text-muted: #64748b;
            --border-color: #e2e8f0;
            --accent: #3b82f6;
            --accent-hover: #2563eb;
            --code-bg: #1e293b;
            --math-bg: #f1f5f9;
        }

        [data-theme="dark"] {
            --bg-primary: #0f172a;
            --bg-secondary: #1e293b;
            --bg-card: #1e293b;
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #94a3b8;
            --border-color: #334155;
            --accent: #3b82f6;
            --accent-hover: #60a5fa;
            --code-bg: #0f172a;
            --math-bg: #1e293b;
        }

        * {
            transition: background-color 0.3s ease, color 0.3s ease, border-color 0.3s ease;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
        }

        .bg-card {
            background-color: var(--bg-card);
            border: 1px solid var(--border-color);
        }

        .bg-secondary {
            background-color: var(--bg-secondary);
        }

        .text-secondary {
            color: var(--text-secondary);
        }

        .text-muted {
            color: var(--text-muted);
        }

        .nav-link {
            position: relative;
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            transition: all 0.3s ease;
        }

        .nav-link:hover {
            background-color: var(--accent);
            color: white;
        }

        .nav-link.active {
            background-color: var(--accent);
            color: white;
        }

        .content-section {
            margin-bottom: 3rem;
            padding: 2rem;
            border-radius: 1rem;
            background-color: var(--bg-card);
            border: 1px solid var(--border-color);
            opacity: 1;
            transform: translateY(0);
            transition: opacity 0.6s ease-out, transform 0.6s ease-out;
        }

        .content-section.animate-in {
            animation: fadeIn 0.6s ease-out;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .heading-1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 1.5rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .heading-2 {
            font-size: 2rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--accent);
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 0.5rem;
        }

        .heading-3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: var(--text-primary);
        }

        .formula-box {
            background-color: var(--math-bg);
            padding: 1.5rem;
            border-radius: 0.75rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            border-left: 4px solid var(--accent);
        }

        .code-block {
            background-color: var(--code-bg);
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            position: relative;
        }

        .copy-btn {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background-color: var(--accent);
            color: white;
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            cursor: pointer;
            font-size: 0.875rem;
            transition: background-color 0.3s ease;
        }

        .copy-btn:hover {
            background-color: var(--accent-hover);
        }

        .highlight-box {
            background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
            border-left: 4px solid var(--accent);
            padding: 1.5rem;
            border-radius: 0.75rem;
            margin: 1.5rem 0;
        }

        .feature-card {
            background-color: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 1rem;
            padding: 1.5rem;
            transition: all 0.3s ease;
            height: 100%;
        }

        .feature-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);
            border-color: var(--accent);
        }

    
        .progress-bar {
            position: fixed;
            top: 0;
            left: 0;
            height: 4px;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            z-index: 1000;
            transition: width 0.3s ease;
        }

        .theme-toggle {
            position: fixed;
            top: 2rem;
            right: 2rem;
            z-index: 1000;
            background-color: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 2rem;
            padding: 0.5rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
        }

  
        .back-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            background-color: var(--accent);
            color: white;
            border: none;
            border-radius: 50%;
            width: 3rem;
            height: 3rem;
            cursor: pointer;
            opacity: 0;
            transition: all 0.3s ease;
            z-index: 1000;
        }

        .back-to-top.show {
            opacity: 1;
        }

        .back-to-top:hover {
            background-color: var(--accent-hover);
            transform: translateY(-4px);
        }



        /* Prism.js Dark Theme Override */
        [data-theme="dark"] pre[class*="language-"] {
            background-color: var(--code-bg);
            color: #f1f5f9;
        }

        [data-theme="dark"] code[class*="language-"] {
            color: #f1f5f9;
        }

        /* MathJax styling */
        .MathJax {
            font-size: 1.1em !important;
        }

        .MathJax_Display {
            margin: 1.5rem 0 !important;
        }

        /* Scrollbar styling */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--text-muted);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-secondary);
        }
    </style>
</head>
<body>
    <!-- Progress Bar -->
    <div class="progress-bar" id="progressBar"></div>

    <!-- Theme Toggle -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon" id="themeIcon"></i>
    </button>

    <!-- Main Content -->
    <main class="container mx-auto px-4 py-8 max-w-4xl">
        <!-- Chapter Header -->
        <header class="text-center mb-12">
            <h1 class="heading-1">第三章 大语言模型基础</h1>
            <p class="text-xl text-secondary mt-4 max-w-3xl mx-auto">
                前两章分别介绍了智能体的定义和发展历史，本章将完全聚焦于大语言模型本身解答一个关键问题：现代智能体是如何工作的？我们将从语言模型的基本定义出发，通过对这些原理的学习，为理解LLM如何获得强大的知识储备与推理能力打下坚实的基础。
            </p>
        </header>

        <!-- Section 3.1 -->
        <section class="content-section" id="section-3-1">
            <h2 class="heading-2">
                <i class="fas fa-brain mr-2"></i>3.1 语言模型与 Transformer 架构
            </h2>

            <!-- Subsection 3.1.1 -->
            <div class="mb-8" id="section-3-1-1">
                <h3 class="heading-3">3.1.1 从 N-gram 到 RNN</h3>

                <div class="highlight-box">
                    <h4 class="font-semibold text-lg mb-3">语言模型 (Language Model, LM)</h4>
                    <p class="text-secondary">
                        是自然语言处理的核心，其根本任务是计算一个词序列（即一个句子）出现的概率。一个好的语言模型能够告诉我们什么样的句子是通顺的、自然的。在多智能体系统中，语言模型是智能体理解人类指令、生成回应的基础。
                    </p>
                </div>

                <h4 class="font-semibold text-lg mt-6 mb-3">（1）统计语言模型与N-gram的思想</h4>
                <p class="mb-4">
                    在深度学习兴起之前，统计方法是语言模型的主流。其核心思想是，一个句子出现的概率，等于该句子中每个词出现的条件概率的连乘。对于一个由词 $w_1,w_2,\dots,w_m$ 构成的句子 S，其概率 P(S) 可以表示为：
                </p>

                <div class="formula-box">
                    $$P(S)=P(w_1,w_2,\dots,w_m)=P(w_1)\cdot P(w_2|w_1)\cdot P(w_3|w_1,w_2)\cdots P(w_m|w_1,\dots,w_{m-1})$$
                </div>

                <p class="mb-4">
                    为了解决这个问题，研究者引入了<strong>马尔可夫假设 (Markov Assumption)</strong> 。其核心思想是：我们不必回溯一个词的全部历史，可以近似地认为，一个词的出现概率只与它前面有限的 $n-1$ 个词有关。
                </p>
                <div class="bg-gradient-to-r from-purple-50 to-blue-50 dark:from-gray-700 dark:to-gray-800 rounded-xl p-6 mb-6">
                        <div class="flex items-center mb-4">
                            <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-0.png"
                                 alt="马尔可夫假设示意图" class="w-full max-w-md rounded-lg shadow-md mx-auto">
                        </div>
                        <p class="text-center text-gray-600 dark:text-gray-300 font-medium">
                            图 3.1 马尔可夫假设示意图
                        </p>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3">Bigram (当 N=2 时)</h5>
                    <p class="text-sm text-secondary mb-2">我们假设一个词的出现只与它前面的一个词有关：</p>
                    <div class="formula-box">
                        $$P(w_i|w_1,\dots,w_{i-1})\approx P(w_i|w_{i-1})$$
                    </div>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3">Trigram (当 N=3 时)</h5>
                    <p class="text-sm text-secondary mb-2">我们假设一个词的出现只与它前面的两个词有关：</p>
                    <div class="formula-box">
                        $$P(w_i|w_1,\dots,w_{i-1})\approx P(w_i|w_{i-2},w_{i-1})$$
                    </div>
                </div>

                <div class="highlight-box mt-6">
                    <h5 class="font-semibold mb-3">最大似然估计 (MLE)</h5>
                    <p class="text-secondary mb-2">这些概率可以通过在大型语料库中进行最大似然估计来计算：</p>
                    <div class="formula-box">
                        $$P(w_i|w_{i-1})=\frac{Count(w_{i-1},w_i)}{Count(w_{i-1})}$$
                    </div>
                    <p class="text-sm text-secondary">
                        其中，$Count(w_{i-1},w_i)$ 表示词对 $(w_{i-1},w_i)$ 在语料库中连续出现的总次数，$Count(w_{i-1})$ 表示单个词 $w_{i-1}$ 在语料库中出现的总次数。
                    </p>
                </div>

                <div class="code-block">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">import collections

# 示例语料库
corpus = "datawhale agent learns datawhale agent works"
tokens = corpus.split()
total_tokens = len(tokens)

# 计算概率
count_datawhale = tokens.count('datawhale')
p_datawhale = count_datawhale / total_tokens

# 构建 bigrams
bigrams = zip(tokens, tokens[1:])
bigram_counts = collections.Counter(bigrams)

# 计算条件概率
count_datawhale_agent = bigram_counts[('datawhale', 'agent')]
p_agent_given_datawhale = count_datawhale_agent / count_datawhale

count_agent_learns = bigram_counts[('agent', 'learns')]
count_agent = tokens.count('agent')
p_learns_given_agent = count_agent_learns / count_agent

# 最终概率
p_sentence = p_datawhale * p_agent_given_datawhale * p_learns_given_agent
print(f"P('datawhale agent learns') ≈ {p_sentence:.3f}")</code></pre>
                </div>

                <div class="feature-card mt-6">
                    <h5 class="font-semibold mb-3 text-red-600">N-gram 模型的缺陷</h5>
                    <ol class="list-decimal list-inside space-y-2 text-secondary">
                        <li><strong>数据稀疏性 (Sparsity)</strong>：如果一个词序列从未在语料库中出现，其概率估计就为 0</li>
                        <li><strong>泛化能力差</strong>：模型无法理解词与词之间的语义相似性</li>
                    </ol>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（2）神经网络语言模型与词嵌入</h4>
                <p class="mb-4">
                    N-gram 模型的根本缺陷在于它将词视为孤立、离散的符号。为了克服这个问题，研究者们转向了神经网络，并提出了一种思想：用连续的向量来表示词。
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">核心思想</h5>
                    <ol class="list-decimal list-inside space-y-2 text-secondary">
                        <li><strong>构建语义空间</strong>：创建高维的连续向量空间，将每个词映射为空间中的一个点（词嵌入）</li>
                        <li><strong>学习映射关系</strong>：利用神经网络学习从上下文到下一个词的映射</li>
                    </ol>
                </div>

                <div class="bg-gradient-to-r from-purple-50 to-blue-50 dark:from-gray-700 dark:to-gray-800 rounded-xl p-6 mb-6">
                        <div class="flex items-center mb-4">
                            <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-1.png"
                                 alt="神经网络语言模型架构示意图" class="w-full max-w-md rounded-lg shadow-md mx-auto">
                        </div>
                        <p class="text-center text-gray-600 dark:text-gray-300 font-medium">
                            图 3.2 神经网络语言模型架构示意图
                        </p>
                </div>

                <div class="formula-box mt-4">
                    <h5 class="font-semibold mb-2">余弦相似度 (Cosine Similarity)</h5>
                    $$\text{similarity}(\vec{a}, \vec{b}) = \cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}| |\vec{b}|}$$
                </div>

                <div class="code-block mt-4">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">import numpy as np

# 假设我们已经学习到了简化的二维词向量
embeddings = {
    "king": np.array([0.9, 0.8]),
    "queen": np.array([0.9, 0.2]),
    "man": np.array([0.7, 0.9]),
    "woman": np.array([0.7, 0.3])
}

def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)
    return dot_product / norm_product

# king - man + woman
result_vec = embeddings["king"] - embeddings["man"] + embeddings["woman"]

# 计算结果向量与 "queen" 的相似度
sim = cosine_similarity(result_vec, embeddings["queen"])

print(f"king - man + woman 的结果向量: {result_vec}")
print(f"该结果与 'queen' 的相似度: {sim:.4f}")</code></pre>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（3）循环神经网络 (RNN) 与长短时记忆网络 (LSTM)</h4>
                <p class="mb-4">
                    为了打破固定窗口的限制，<strong>循环神经网络 (Recurrent Neural Network, RNN)</strong> 应运而生，其核心思想非常直观：为网络增加"记忆"能力。
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">RNN 的创新</h5>
                    <p class="text-secondary">
                        引入了<strong>隐藏状态 (hidden state)</strong> 向量，可以理解为网络的短期记忆。在处理序列的每一步，网络都会读取当前的输入词，并结合它上一刻的记忆，然后生成一个新的记忆传递给下一刻。
                    </p>
                </div>

                <div class="bg-gradient-to-r from-purple-50 to-blue-50 dark:from-gray-700 dark:to-gray-800 rounded-xl p-6 mb-6">
                        <div class="flex items-center mb-4">
                            <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-2.png"
                                 alt="RNN 结构示意图" class="w-full max-w-md rounded-lg shadow-md mx-auto">
                        </div>
                        <p class="text-center text-gray-600 dark:text-gray-300 font-medium">
                            图 3.3 RNN 结构示意图
                        </p>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-yellow-600">长期依赖问题</h5>
                    <p class="text-secondary">
                        标准的 RNN 在实践中存在严重问题：当序列很长时，梯度在反向传播过程中会出现<strong>梯度消失</strong>或<strong>梯度爆炸</strong>，使得模型难以捕捉长距离依赖关系。
                    </p>
                </div>

                <div class="highlight-box mt-4">
                    <h5 class="font-semibold mb-3">LSTM 的解决方案</h5>
                    <p class="text-secondary mb-3">
                        <strong>长短时记忆网络 (LSTM)</strong> 引入了<strong>细胞状态</strong>和<strong>门控机制</strong>：
                    </p>
                    <ul class="list-disc list-inside space-y-1 text-secondary">
                        <li><strong>遗忘门 (Forget Gate)</strong>：决定从上一时刻的细胞状态中丢弃哪些信息</li>
                        <li><strong>输入门 (Input Gate)</strong>：决定将当前输入中的哪些新信息存入细胞状态</li>
                        <li><strong>输出门 (Output Gate)</strong>：决定根据当前的细胞状态，输出哪些信息到隐藏状态</li>
                    </ul>
                </div>
            </div>

            <!-- Subsection 3.1.2 -->
            <div class="mb-8" id="section-3-1-2">
                <h3 class="heading-3">3.1.2 Transformer 架构解析</h3>

                <p class="mb-4">
                    Transformer 在2017年由谷歌团队提出，它完全抛弃了循环结构，转而完全依赖一种名为<strong>注意力 (Attention)</strong> 的机制来捕捉序列内的依赖关系，从而实现了真正意义上的并行计算。
                </p>

                <h4 class="font-semibold text-lg mt-6 mb-3">（1）Encoder-Decoder 整体结构</h4>
                <p class="mb-4">
                    最初的 Transformer 模型遵循<strong>编码器-解码器 (Encoder-Decoder)</strong> 架构：
                </p>

                <div class="bg-gradient-to-r from-purple-50 to-blue-50 dark:from-gray-700 dark:to-gray-800 rounded-xl p-6 mb-6">
                        <div class="flex items-center mb-4">
                            <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-3.png"
                                 alt="Transformer 整体架构图" class="w-full max-w-md rounded-lg shadow-md mx-auto">
                        </div>
                        <p class="text-center text-gray-600 dark:text-gray-300 font-medium">
                            图 3.4 Transformer 整体架构图
                        </p>
                </div>

                <div class="grid md:grid-cols-2 gap-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-blue-600">
                            <i class="fas fa-lock mr-2"></i>编码器 (Encoder)
                        </h5>
                        <p class="text-sm text-secondary">
                            任务是"理解"输入的整个句子。它会读取所有输入词元，最终为每个词元生成一个富含上下文信息的向量表示。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-green-600">
                            <i class="fas fa-unlock mr-2"></i>解码器 (Decoder)
                        </h5>
                        <p class="text-sm text-secondary">
                            任务是"生成"目标句子。它会参考自己已经生成的前文，并"咨询"编码器的理解结果，来生成下一个词。
                        </p>
                    </div>
                </div>

                <div class="code-block mt-6">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """位置编码模块"""
    def forward(self, x):
        pass

class MultiHeadAttention(nn.Module):
    """多头注意力机制模块"""
    def forward(self, query, key, value, mask):
        pass

class PositionWiseFeedForward(nn.Module):
    """位置前馈网络模块"""
    def forward(self, x):
        pass

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention()
        self.feed_forward = PositionWiseFeedForward()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # 1. 多头自注意力
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x</code></pre>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（2）从自注意力到多头注意力</h4>
                <p class="mb-4">
                    <strong>自注意力 (Self-Attention)</strong> 机制允许模型在处理序列中的每一个词时，都能兼顾句子中的所有其他词，并为这些词分配不同的"注意力权重"。
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">三个角色</h5>
                    <ul class="list-disc list-inside space-y-2 text-secondary">
                        <li><strong>查询 (Query, Q)</strong>：代表当前词元，正在主动地"查询"其他词元以获取信息</li>
                        <li><strong>键 (Key, K)</strong>：代表句子中可被查询的词元"标签"或"索引"</li>
                        <li><strong>值 (Value, V)</strong>：代表词元本身所携带的"内容"或"信息"</li>
                    </ul>
                </div>

                <div class="formula-box">
                    <h5 class="font-semibold mb-2">注意力计算公式</h5>
                    $$\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$$
                </div>

                <div class="bg-gradient-to-r from-purple-50 to-blue-50 dark:from-gray-700 dark:to-gray-800 rounded-xl p-6 mb-6">
                        <div class="flex items-center mb-4">
                            <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-4.png"
                                 alt="多头注意力机制" class="w-full max-w-md rounded-lg shadow-md mx-auto">
                        </div>
                        <p class="text-center text-gray-600 dark:text-gray-300 font-medium">
                            图 3.5 多头注意力机制
                        </p>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3">多头注意力 (Multi-Head Attention)</h5>
                    <p class="text-secondary">
                        把一次做完变成分成几组，分开做，再合并。将原始的 Q, K, V 向量在维度上切分成 h 份，每一份都独立地进行一次单头注意力的计算，最后将结果拼接起来。
                    </p>
                </div>

                <div class="code-block mt-4">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">class MultiHeadAttention(nn.Module):
    """多头注意力机制模块"""
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model 必须能被 num_heads 整除"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # 定义 Q, K, V 和输出的线性变换层
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # 1. 计算注意力得分 (QK^T)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # 2. 应用掩码 (如果提供)
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

        # 3. 计算注意力权重 (Softmax)
        attn_probs = torch.softmax(attn_scores, dim=-1)

        # 4. 加权求和 (权重 * V)
        output = torch.matmul(attn_probs, V)
        return output</code></pre>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（3）前馈神经网络</h4>
                <p class="mb-4">
                    在每个 Encoder 和 Decoder 层中，多头注意力子层之后都跟着一个<strong>逐位置前馈网络(Position-wise Feed-Forward Network, FFN)</strong>。
                </p>

                <div class="formula-box">
                    $$\mathrm{FFN}(x)=\max\left(0, xW_{1}+b_{1}\right) W_{2}+b_{2}$$
                </div>

                <div class="code-block mt-4">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">class PositionWiseFeedForward(nn.Module):
    """位置前馈网络模块"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionWiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x 形状: (batch_size, seq_len, d_model)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x</code></pre>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（4）残差连接与层归一化</h4>
                <p class="mb-4">
                    在 Transformer 的每个编码器和解码器层中，所有子模块都被一个 `Add & Norm` 操作包裹：
                </p>

                <div class="grid md:grid-cols-2 gap-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-purple-600">
                            <i class="fas fa-plus-circle mr-2"></i>残差连接 (Add)
                        </h5>
                        <p class="text-sm text-secondary">
                            将子模块的输入直接加到输出上，解决深度神经网络中的梯度消失问题：$\text{Output} = x + \text{Sublayer}(x)$
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-indigo-600">
                            <i class="fas fa-chart-line mr-2"></i>层归一化 (Norm)
                        </h5>
                        <p class="text-sm text-secondary">
                            对单个样本的所有特征进行归一化，使其均值为0，方差为1，解决内部协变量偏移问题。
                        </p>
                    </div>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（5）位置编码</h4>
                <p class="mb-4">
                    Transformer 的自注意力机制本身不包含位置信息，因此引入了<strong>位置编码 (Positional Encoding)</strong>：
                </p>

                <div class="formula-box">
                    <h5 class="font-semibold mb-2">位置编码公式</h5>
                    $$PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
                    $$PE_{(pos,2i+1)}=\cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
                </div>

                <div class="code-block mt-4">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">class PositionalEncoding(nn.Module):
    """为输入序列的词嵌入向量添加位置编码"""
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # 创建一个足够长的位置编码矩阵
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        pe = torch.zeros(max_len, 1, d_model)

        # 偶数维度使用 sin, 奇数维度使用 cos
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)</code></pre>
                </div>
            </div>

            <!-- Subsection 3.1.3 -->
            <div class="mb-8" id="section-3-1-3">
                <h3 class="heading-3">3.1.3 Decoder-Only 架构</h3>

                <p class="mb-4">
                    OpenAI 在开发 <strong>GPT (Generative Pre-trained Transformer)</strong> 时，提出了一个更简单的思想：<strong>语言的核心任务，不就是预测下一个最有可能出现的词吗？</strong>
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">自回归 (Autoregressive) 工作模式</h5>
                    <ol class="list-decimal list-inside space-y-2 text-secondary">
                        <li>给模型一个起始文本</li>
                        <li>模型预测出下一个最有可能的词</li>
                        <li>模型将自己刚刚生成的词添加到输入文本的末尾</li>
                        <li>模型基于这个新输入，再次预测下一个词</li>
                        <li>不断重复这个过程，直到生成完整的句子或达到停止条件</li>
                    </ol>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-orange-600">掩码自注意力 (Masked Self-Attention)</h5>
                    <p class="text-secondary">
                        在 Decoder-Only 架构中，这个机制至关重要。它通过掩码将所有位于当前位置之后的词元对应的分数替换为一个非常大的负数，使得模型在预测下一个词时，能且仅能依赖它已经见过的信息。
                    </p>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-green-600">Decoder-Only 架构的优势</h5>
                    <ul class="list-disc list-inside space-y-1 text-secondary">
                        <li><strong>训练目标统一</strong>：模型的唯一任务就是"预测下一个词"</li>
                        <li><strong>结构简单，易于扩展</strong>：更少的组件意味着更容易进行规模化扩展</li>
                        <li><strong>天然适合生成任务</strong>：自回归模式与所有生成式任务完美契合</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Section 3.2 -->
        <section class="content-section" id="section-3-2">
            <h2 class="heading-2">
                <i class="fas fa-comments mr-2"></i>3.2 与大语言模型交互
            </h2>

            <!-- Subsection 3.2.1 -->
            <div class="mb-8" id="section-3-2-1">
                <h3 class="heading-3">3.2.1 提示工程</h3>

                <p class="mb-4">
                    如果我们把大语言模型比作一个能力极强的"大脑"，那么<strong>提示 (Prompt)</strong> 就是我们与这个"大脑"沟通的语言。提示工程，就是研究如何设计出精准的提示，从而引导模型产生我们期望输出的回复。
                </p>

                <h4 class="font-semibold text-lg mt-6 mb-3">（1）模型采样参数</h4>
                <p class="mb-4">
                    在使用大模型时，你会经常看到类似`Temperature`这类的可配置参数，其本质是通过调整模型对"概率分布"的采样策略，让输出匹配具体场景需求。
                </p>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-red-600">Temperature</h5>
                    <p class="text-secondary mb-2">
                        温度是控制模型输出"随机性"与"确定性"的关键参数：
                    </p>
                    <div class="formula-box">
                        $$p_i^{(T)} = \frac{e^{z_i / T}}{\sum_{j=1}^k e^{z_j / T}}$$
                    </div>
                    <div class="grid md:grid-cols-3 gap-3 mt-3">
                        <div class="bg-red-50 dark:bg-red-900/20 p-3 rounded-lg">
                            <h6 class="font-semibold text-red-700 dark:text-red-400">低温度 (0 ≤ T < 0.3)</h6>
                            <p class="text-xs text-secondary mt-1">输出更"精准、确定"</p>
                            <p class="text-xs text-muted">适用：事实性任务、严谨性场景</p>
                        </div>
                        <div class="bg-yellow-50 dark:bg-yellow-900/20 p-3 rounded-lg">
                            <h6 class="font-semibold text-yellow-700 dark:text-yellow-400">中温度 (0.3 ≤ T < 0.7)</h6>
                            <p class="text-xs text-secondary mt-1">输出"平衡、自然"</p>
                            <p class="text-xs text-muted">适用：日常对话、常规创作</p>
                        </div>
                        <div class="bg-green-50 dark:bg-green-900/20 p-3 rounded-lg">
                            <h6 class="font-semibold text-green-700 dark:text-green-400">高温度 (0.7 ≤ T < 2)</h6>
                            <p class="text-xs text-secondary mt-1">输出"创新、发散"</p>
                            <p class="text-xs text-muted">适用：创意性任务、发散性思考</p>
                        </div>
                    </div>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-blue-600">Top-k</h5>
                    <p class="text-secondary mb-2">
                        将所有 token 按概率从高到低排序，取排名前 k 个的 token 组成"候选集"：
                    </p>
                    <div class="formula-box">
                        $$\hat{p}_i = \frac{p_i}{\sum_{j \in \text{候选集}} p_j}$$
                    </div>
                    <p class="text-sm text-secondary">
                        当k=1时输出完全确定，退化为"贪心采样"
                    </p>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-purple-600">Top-p</h5>
                    <p class="text-secondary mb-2">
                        从排序后的第一个 token 开始，逐步累加概率，直到累积和首次达到或超过阈值 p：
                    </p>
                    <div class="formula-box">
                        $$\sum_{i \in S} p_{(i)} \geq p$$
                    </div>
                    <p class="text-sm text-secondary">
                        相对于固定截断大小的 Top-k，Top-p 能动态适应不同分布的"长尾"特性
                    </p>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（2）零样本、单样本与少样本提示</h4>
                <p class="mb-4">
                    根据我们给模型提供示例（Exemplar）的数量，提示可以分为三种类型：
                </p>

                <div class="space-y-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-indigo-600">
                            <i class="fas fa-bolt mr-2"></i>零样本提示 (Zero-shot Prompting)
                        </h5>
                        <p class="text-secondary mb-2">不给模型任何示例，直接让它根据指令完成任务：</p>
                        <div class="code-block">
                            <pre><code class="language-plain">文本：Datawhale的AI Agent课程非常棒！
情感：正面</code></pre>
                        </div>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-teal-600">
                            <i class="fas fa-fingerprint mr-2"></i>单样本提示 (One-shot Prompting)
                        </h5>
                        <p class="text-secondary mb-2">给模型提供一个完整的示例作为示范：</p>
                        <div class="code-block">
                            <pre><code class="language-plain">文本：这家餐厅的服务太慢了。
情感：负面

文本：Datawhale的AI Agent课程非常棒！
情感：</code></pre>
                        </div>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-green-600">
                            <i class="fas fa-layer-group mr-2"></i>少样本提示 (Few-shot Prompting)
                        </h5>
                        <p class="text-secondary mb-2">提供多个示例，让模型对任务有更全面的理解：</p>
                        <div class="code-block">
                            <pre><code class="language-plain">文本：这家餐厅的服务太慢了。
情感：负面

文本：这部电影的情节很平淡。
情感：中性

文本：Datawhale的AI Agent课程非常棒！
情感：</code></pre>
                        </div>
                    </div>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（3）指令调优的影响</h4>
                <p class="mb-4">
                    <strong>指令调优 (Instruction Tuning)</strong> 是一种微调技术，它使用大量"指令-回答"格式的数据对预训练模型进行进一步的训练。经过指令调优后，模型能更好地理解并遵循用户的指令。
                </p>

                <div class="grid md:grid-cols-2 gap-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-orange-600">文本补全模型</h5>
                        <p class="text-sm text-secondary mb-2">需要用少样本提示"教会"模型：</p>
                        <div class="code-block">
                            <pre><code class="language-plain">这是一段将英文翻译成中文的程序。
英文：Hello
中文：你好
英文：How are you?
中文：</code></pre>
                        </div>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-green-600">指令调优模型</h5>
                        <p class="text-sm text-secondary mb-2">可以直接下达指令：</p>
                        <div class="code-block">
                            <pre><code class="language-plain">请将下面的英文翻译成中文：
How are you?</code></pre>
                        </div>
                    </div>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（4）基础提示技巧</h4>

                <div class="space-y-4 mt-4">
                    <div class="highlight-box">
                        <h5 class="font-semibold mb-3 text-purple-600">
                            <i class="fas fa-theater-masks mr-2"></i>角色扮演 (Role-playing)
                        </h5>
                        <p class="text-secondary mb-2">通过赋予模型一个特定的角色，引导它的回答风格、语气和知识范围：</p>
                        <div class="code-block">
                            <pre><code class="language-plain">你现在是一位资深的Python编程专家。请解释一下Python中的GIL（全局解释器锁）是什么，要让一个初学者也能听懂。</code></pre>
                        </div>
                    </div>

                    <div class="highlight-box">
                        <h5 class="font-semibold mb-3 text-blue-600">
                            <i class="fas fa-list-alt mr-2"></i>上下文示例 (In-context Example)
                        </h5>
                        <p class="text-secondary mb-2">通过在提示中提供清晰的输入输出示例，"教会"模型如何处理请求：</p>
                        <div class="code-block">
                            <pre><code class="language-plain">我需要你从产品评论中提取产品名称和用户情感。请严格按照下面的JSON格式输出。

评论：这款"星尘"笔记本电脑的屏幕显示效果惊人，但我不太喜欢它的键盘手感。
输出：{"product_name": "星尘笔记本电脑", "sentiment": "混合"}

评论：我刚买的"声动"耳机音质很棒，续航也超出了我的预期！
输出：</code></pre>
                        </div>
                    </div>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">（5）思维链</h4>
                <p class="mb-4">
                    对于需要逻辑推理、计算或多步骤思考的复杂问题，<strong>思维链 (Chain-of-Thought, CoT)</strong> 是一种强大的提示技巧，它通过引导模型"一步一步地思考"，提升了模型在复杂任务上的推理能力。
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">实现方法</h5>
                    <p class="text-secondary mb-2">
                        在提示中加入简单的引导语，如"请逐步思考"或"Let's think step by step"：
                    </p>
                    <div class="code-block">
                        <pre><code class="language-plain">一个篮球队在一个赛季的80场比赛中赢了60%。在接下来的赛季中，他们打了15场比赛，赢了12场。两个赛季的总胜率是多少？
请一步一步地思考并解答。

>>>
（模型可能会输出）
好的，我们来一步步计算。
第一步：计算第一个赛季赢得的比赛数。
80场 * 60% = 48场。
第二步：计算两个赛季的总比赛数和总胜利数。
总比赛数 = 80 + 15 = 95场。
总胜利数 = 48 + 12 = 60场。
第三步：计算总胜率。
总胜率 = (总胜利数 / 总比赛数) * 100% = (60 / 95) * 100% ≈ 63.16%。
所以，两个赛季的总胜率约为63.16%。</code></pre>
                    </div>
                </div>
            </div>

            <!-- Subsection 3.2.2 -->
            <div class="mb-8" id="section-3-2-2">
                <h3 class="heading-3">3.2.2 文本分词</h3>

                <p class="mb-4">
                    将文本序列转换为数字序列的过程，就叫做<strong>分词 (Tokenization)</strong> 。<strong>分词器 (Tokenizer)</strong> 的作用，就是定义一套规则，将原始文本切分成一个个最小的单元，我们称之为<strong>词元 (Token)</strong> 。
                </p>

                <h4 class="font-semibold text-lg mt-6 mb-3">3.2.2.1 为何需要分词</h4>

                <div class="grid md:grid-cols-3 gap-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-red-600">
                            <i class="fas fa-font mr-2"></i>按词分词 (Word-based)
                        </h5>
                        <p class="text-sm text-secondary">
                            直接用空格或标点符号将句子切分成单词。面临"词表爆炸"和OOV问题。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-yellow-600">
                            <i class="fas fa-text-height mr-2"></i>按字符分词 (Character-based)
                        </h5>
                        <p class="text-sm text-secondary">
                            将文本切分成单个字符。词表小，不存在OOV问题，但学习效率低下。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-green-600">
                            <i class="fas fa-puzzle-piece mr-2"></i>子词分词 (Subword)
                        </h5>
                        <p class="text-sm text-secondary">
                            兼顾词表大小和语义表达，将常见词保留为完整词元，不常见词拆分成子词片段。
                        </p>
                    </div>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">3.2.2.2 字节对编码算法解析</h4>
                <p class="mb-4">
                    <strong>字节对编码 (Byte-Pair Encoding, BPE)</strong> 是最主流的子词分词算法之一，其核心思想是一个"贪心"的合并过程：
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">BPE 算法步骤</h5>
                    <ol class="list-decimal list-inside space-y-2 text-secondary">
                        <li><strong>初始化</strong>：将词表初始化为所有在语料库中出现过的基本字符</li>
                        <li><strong>迭代合并</strong>：统计所有相邻词元对的出现频率，找到频率最高的一对进行合并</li>
                        <li><strong>重复</strong>：重复第2步，直到词表大小达到预设的阈值</li>
                    </ol>
                </div>

                <div class="bg-gradient-to-r from-purple-50 to-blue-50 dark:from-gray-700 dark:to-gray-800 rounded-xl p-6 mb-6">
                        <div class="flex items-center mb-4">
                            <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/3-figures/1757249275674-5.png"
                                 alt="BPE 算法合并过程示例" class="w-full max-w-md rounded-lg shadow-md mx-auto">
                        </div>
                        <p class="text-center text-gray-600 dark:text-gray-300 font-medium">
                            表 3.1 BPE 算法合并过程示例
                        </p>
                </div>

                <div class="code-block mt-4">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">import re, collections

def get_stats(vocab):
    """统计词元对频率"""
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    """合并词元对"""
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

# 准备语料库
vocab = {'h u g </w>': 1, 'p u g </w>': 1, 'p u n </w>': 1, 'b u n </w>': 1}
num_merges = 4

for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(f"第{i+1}次合并: {best} -> {''.join(best)}")</code></pre>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3">其他分词算法</h5>
                    <ul class="list-disc list-inside space-y-2 text-secondary">
                        <li><strong>WordPiece</strong>：Google BERT 采用，合并标准是"能最大化提升语料库的语言模型概率"</li>
                        <li><strong>SentencePiece</strong>：Google 开源，将空格也视作普通字符，完全可逆且不依赖特定语言</li>
                    </ul>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">3.2.2.3 分词器对开发者的意义</h4>

                <div class="space-y-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-blue-600">
                            <i class="fas fa-window-maximize mr-2"></i>上下文窗口限制
                        </h5>
                        <p class="text-secondary">
                            模型的上下文窗口（如 8K, 128K）是以 <strong>Token 数量</strong>计算的，而不是字符数。精确管理输入长度、避免超出上下文限制是构建长时记忆智能体的基础。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-green-600">
                            <i class="fas fa-dollar-sign mr-2"></i>API 成本
                        </h5>
                        <p class="text-secondary">
                            大多数模型 API 都是按 Token 数量计费的。了解你的文本会被如何分词，是预估和控制智能体运行成本的关键一步。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-purple-600">
                            <i class="fas fa-exclamation-triangle mr-2"></i>模型表现的异常
                        </h5>
                        <p class="text-secondary">
                            模型奇怪表现的根源可能在于分词。例如，模型可能擅长计算 `2 + 2`，但对于 `2+2`（没有空格）就可能出错，因为可能被分词器视为不同的词元。
                        </p>
                    </div>
                </div>
            </div>

            <!-- Subsection 3.2.3 -->
            <div class="mb-8" id="section-3-2-3">
                <h3 class="heading-3">3.2.3 调用开源大语言模型</h3>

                <p class="mb-4">
                    <strong>Hugging Face Transformers</strong> 是一个强大的开源库，它提供了标准化的接口来加载和使用数以万计的预训练模型。
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">配置环境与选择模型</h5>
                    <p class="text-secondary mb-2">
                        我们选择了一个小规模但功能强大的模型：`Qwen/Qwen1.5-0.5B-Chat`。首先安装必要的库：
                    </p>
                    <div class="code-block">
                        <pre><code class="language-plain">pip install transformers torch</code></pre>
                    </div>
                </div>

                <div class="code-block mt-4">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python">import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# 指定模型ID
model_id = "Qwen/Qwen1.5-0.5B-Chat"

# 设置设备，优先使用GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 加载分词器
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 加载模型，并将其移动到指定设备
model = AutoModelForCausalLM.from_pretrained(model_id).to(device)

print("模型和分词器加载完成！")</code></pre>
                </div>

                <div class="code-block mt-4">
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="fas fa-copy mr-2"></i>复制
                    </button>
                    <pre><code class="language-python"># 准备对话输入
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "你好，请介绍你自己。"}
]

# 使用分词器的模板格式化输入
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# 编码输入文本
model_inputs = tokenizer([text], return_tensors="pt").to(device)

# 使用模型生成回答
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)

# 解码生成的 Token ID
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

print("\n模型的回答:")
print(response)</code></pre>
                </div>
            </div>

            <!-- Subsection 3.2.4 -->
            <div class="mb-8" id="section-3-2-4">
                <h3 class="heading-3">3.2.4 模型的选择</h3>

                <p class="mb-4">
                    选择语言模型并非简单地追求"最大、最强"，而是一个在性能、成本、速度和部署方式之间进行权衡的决策过程。
                </p>

                <h4 class="font-semibold text-lg mt-6 mb-3">3.2.4.1 模型选型的关键考量</h4>

                <div class="grid md:grid-cols-2 gap-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-blue-600">
                            <i class="fas fa-tachometer-alt mr-2"></i>性能与能力
                        </h5>
                        <p class="text-sm text-secondary">
                            不同模型擅长的任务不同，可参考公开的基准测试排行榜评估综合能力。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-green-600">
                            <i class="fas fa-dollar-sign mr-2"></i>成本
                        </h5>
                        <p class="text-sm text-secondary">
                            闭源模型按API调用费用计费，开源模型成本体现在本地部署的硬件和运维上。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-purple-600">
                            <i class="fas fa-bolt mr-2"></i>速度（延迟）
                        </h5>
                        <p class="text-sm text-secondary">
                            对于实时交互的智能体，响应速度至关重要。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-orange-600">
                            <i class="fas fa-expand-arrows-alt mr-2"></i>上下文窗口
                        </h5>
                        <p class="text-sm text-secondary">
                            模型能一次性处理的Token数量上限，对于长文档分析、代码库理解等场景很重要。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-red-600">
                            <i class="fas fa-server mr-2"></i>部署方式
                        </h5>
                        <p class="text-sm text-secondary">
                            API方式简单但数据需发送给第三方，本地部署确保数据隐私但技术要求更高。
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-indigo-600">
                            <i class="fas fa-puzzle-piece mr-2"></i>生态与工具链
                        </h5>
                        <p class="text-sm text-secondary">
                            模型的流行程度决定了周边生态的成熟度，包括社区支持、教程、开发框架等。
                        </p>
                    </div>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">3.2.4.2 闭源模型概览</h4>

                <div class="space-y-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-blue-600">
                            <i class="fas fa-robot mr-2"></i>OpenAI GPT 系列
                        </h5>
                        <p class="text-secondary">
                            从开启大模型时代的 GPT-3，到引入 RLHF 的 ChatGPT，再到开启多模态时代的 GPT-4，OpenAI 持续引领行业发展。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-green-600">
                            <i class="fas fa-gem mr-2"></i>Google Gemini 系列
                        </h5>
                        <p class="text-secondary">
                            原生多模态的代表，能统一处理文本、代码、音视频和图像等多种模态的数据，以超长上下文窗口在海量信息处理上具备优势。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-purple-600">
                            <i class="fas fa-brain mr-2"></i>Anthropic Claude 系列
                        </h5>
                        <p class="text-secondary">
                            专注于 AI 安全和负责任 AI，以其在处理长文档、减少有害输出、遵循指令方面的可靠性而闻名。
                        </p>
                    </div>
                </div>

                <h4 class="font-semibold text-lg mt-8 mb-3">3.2.4.3 开源模型概览</h4>

                <div class="space-y-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-orange-600">
                            <i class="fas fa-horse mr-2"></i>Meta Llama 系列
                        </h5>
                        <p class="text-secondary">
                            凭借出色的综合性能、开放的许可协议和强大的社区支持，成为许多衍生项目和研究的基座。Llama 4 系列采用混合专家（MoE）架构。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-teal-600">
                            <i class="fas fa-wind mr-2"></i>Mistral AI 系列
                        </h5>
                        <p class="text-secondary">
                            以"小尺寸、高性能"的设计而闻名，最新模型在代码生成、STEM推理和跨领域问答等任务上准确率与响应速度均有显著提升。
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 3.3 -->
        <section class="content-section" id="section-3-3">
            <h2 class="heading-2">
                <i class="fas fa-chart-line mr-2"></i>3.3 大语言模型的缩放法则与局限性
            </h2>

            <!-- Subsection 3.3.1 -->
            <div class="mb-8" id="section-3-3-1">
                <h3 class="heading-3">3.3.1 缩放法则</h3>

                <p class="mb-4">
                    <strong>缩放法则（Scaling Laws）</strong>是近年来大语言模型领域最重要的发现之一。它揭示了模型性能与模型参数量、训练数据量以及计算资源之间存在着可预测的幂律关系。
                </p>

                <div class="highlight-box">
                    <h5 class="font-semibold mb-3">核心发现</h5>
                    <p class="text-secondary mb-3">
                        在对数-对数坐标系下，模型的性能与参数量、数据量和计算量都呈现出平滑的幂律关系。这意味着只要我们持续、按比例地增加这三个要素，模型的性能就会可预测地、平滑地提升。
                    </p>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-blue-600">Chinchilla 定律</h5>
                        <p class="text-sm text-secondary">
                            在给定计算预算下，模型参数量和训练数据量存在最优配比。最优模型应该比之前认为的要小，但需要用多得多的数据训练。例如，700亿参数的Chinchilla模型使用比GPT-3多4倍的数据训练，性能反而超越了1750亿参数的GPT-3。
                        </p>
                    </div>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-purple-600">
                        <i class="fas fa-magic mr-2"></i>能力的涌现
                    </h5>
                    <p class="text-secondary">
                        当模型规模达到一定阈值后，会突然展现出在小规模模型中完全不存在或表现不佳的全新能力，如链式思考、指令遵循、多步推理、代码生成等。
                    </p>
                </div>
            </div>

            <!-- Subsection 3.3.2 -->
            <div class="mb-8" id="section-3-3-2">
                <h3 class="heading-3">3.3.2 模型幻觉</h3>

                <p class="mb-4">
                    <strong>模型幻觉（Hallucination）</strong>通常指的是大语言模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件。
                </p>

                <div class="grid md:grid-cols-3 gap-4 mt-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-red-600">
                            <i class="fas fa-fact-check mr-2"></i>事实性幻觉
                        </h5>
                        <p class="text-sm text-secondary">
                            模型生成与现实世界事实不符的信息
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-yellow-600">
                            <i class="fas fa-balance-scale mr-2"></i>忠实性幻觉
                        </h5>
                        <p class="text-sm text-secondary">
                            生成的内容未能忠实地反映源文本的含义
                        </p>
                    </div>
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 text-orange-600">
                            <i class="fas fa-exchange-alt mr-2"></i>内在幻觉
                        </h5>
                        <p class="text-sm text-secondary">
                            模型生成的内容与输入信息直接矛盾
                        </p>
                    </div>
                </div>

                <div class="highlight-box mt-6">
                    <h5 class="font-semibold mb-3">幻觉的产生原因</h5>
                    <ul class="list-disc list-inside space-y-2 text-secondary">
                        <li>训练数据中可能包含错误或矛盾的信息</li>
                        <li>模型的自回归生成机制只是在预测下一个最可能的词元，没有内置的事实核查模块</li>
                        <li>在面对需要复杂推理的任务时，模型可能会在逻辑链条中出错</li>
                    </ul>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-green-600">缓解幻觉的方法</h5>
                    <ol class="list-decimal list-inside space-y-2 text-secondary">
                        <li><strong>数据层面</strong>：高质量数据清洗、引入事实性知识、RLHF</li>
                        <li><strong>模型层面</strong>：探索新的模型架构，让模型表达不确定性</li>
                        <li><strong>推理与生成层面</strong>：
                            <ul class="list-circle list-inside mt-2 space-y-1">
                                <li>检索增强生成 (RAG)</li>
                                <li>多步推理与验证</li>
                                <li>引入外部工具</li>
                            </ul>
                        </li>
                    </ol>
                </div>

                <div class="feature-card mt-4">
                    <h5 class="font-semibold mb-3 text-blue-600">其他挑战</h5>
                    <ul class="list-disc list-inside space-y-2 text-secondary">
                        <li><strong>知识时效性不足</strong>：模型知识来源于训练数据，无法感知训练后的新事件</li>
                        <li><strong>偏见问题</strong>：训练数据包含人类社会的各种偏见，模型会吸收并反映这些偏见</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Section 3.4 -->
        <section class="content-section" id="section-3-4">
            <h2 class="heading-2">
                <i class="fas fa-book-open mr-2"></i>3.4 本章小结
            </h2>

            <p class="mb-6 text-secondary">
                本章介绍了构建智能体所需的基础知识，重点围绕作为其核心组件的大语言模型 (LLM) 展开。内容从语言模型的早期发展开始，详细讲解了 Transformer 架构，并介绍了与 LLM 进行交互的方法。
            </p>

            <div class="highlight-box">
                <h4 class="font-semibold text-lg mb-4">核心知识点回顾</h4>
                <div class="space-y-4">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-blue-600">
                            <i class="fas fa-history mr-2"></i>模型演进与核心架构
                        </h5>
                        <p class="text-sm text-secondary">
                            从统计语言模型 (N-gram) 到神经网络模型 (RNN, LSTM)，再到奠定现代 LLM 基础的 Transformer 架构。通过代码实现拆解了 Transformer 的核心组件。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-green-600">
                            <i class="fas fa-comments mr-2"></i>与模型的交互方式
                        </h5>
                        <p class="text-sm text-secondary">
                            提示工程 (Prompt Engineering) 和文本分词 (Tokenization) 是与 LLM 交互的两个核心环节。通过本地部署开源模型的实践，将理论知识应用于实际操作。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-purple-600">
                            <i class="fas fa-robot mr-2"></i>模型生态与选型
                        </h5>
                        <p class="text-sm text-secondary">
                            系统梳理了为智能体选择模型时需要权衡的关键因素，概览了闭源模型和开源模型的特点与定位。
                        </p>
                    </div>

                    <div class="feature-card">
                        <h5 class="font-semibold mb-2 text-orange-600">
                            <i class="fas fa-chart-line mr-2"></i>法则与局限
                        </h5>
                        <p class="text-sm text-secondary">
                            探讨了驱动 LLM 能力提升的缩放法则，分析了模型存在的如事实幻觉、知识过时等固有局限性。
                        </p>
                    </div>
                </div>
            </div>

            <div class="highlight-box mt-6">
                <h4 class="font-semibold text-lg mb-4">从 LLM 基础到构建智能体</h4>
                <p class="text-secondary">
                    这一章的LLM基础主要是为了帮助大家更好的理解大模型的诞生以及发展过程，其中也蕴含了智能体设计的部分思考。例如，如何设计有效的提示词来引导 Agent 的规划与决策，如何根据任务需求选择合适的模型，以及如何在 Agent 的工作流中加入验证机制以规避模型的幻觉等问题，其解决方案均建立在本章的基础之上。
                </p>
                <p class="text-secondary mt-3">
                    我们现在已经准备好从理论转向实践。在下一章，我们将开始探索智能体经典范式构建，将本章所学的知识应用于实际的智能体设计之中。
                </p>
            </div>
        </section>

        <!-- References Section -->
        <section class="content-section" id="references">
            <h2 class="heading-2">
                <i class="fas fa-book mr-2"></i>参考文献
            </h2>

            <div class="space-y-3 text-sm text-secondary">
                <p>[1] Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. <em>Journal of Machine Learning Research</em>, 3, 1137-1155.</p>
                <p>[2] Elman, J. L. (1990). Finding structure in time. <em>Cognitive Science</em>, 14(2), 179-211.</p>
                <p>[3] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, 9(8), 1735-1780.</p>
                <p>[4] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In <em>Advances in neural information processing systems</em> (pp. 5998-6008).</p>
                <p>[5] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. OpenAI.</p>
                <p>[6] Gage, P. (1994). A new algorithm for data compression. <em>C Users Journal</em>, <em>12</em>(2), 23-38.</p>
                <p>[7] Schuster, M., & Nakajima, K. (2012, March). Japanese and korean voice search. In <em>2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em> (pp. 5149-5152). IEEE.</p>
                <p>[8] Kudo, T., & Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. <em>arXiv preprint arXiv:1808.06226</em>.</p>
                <p>[9] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling Laws for Neural Language Models. arXiv preprint arXiv:2001.08361.</p>
                <p>[10] Hoffmann, J., Borgeaud, E., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, R., ... & Sifre, L. (2022). Training Compute-Optimal Large Language Models. <em>arXiv preprint arXiv:2203.07678</em>.</p>
                <p>[11] Ji, Z., Lee, N., Fries, R., Yu, T., & Su, D. (2023). Survey of Hallucination in Large Language Models.</p>
                <p>[12] Bender, E. M., Gebru, T., McMillan-Major, A., & Mitchell, M. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? .</p>
                <p>[13] Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. <em>arXiv preprint arXiv:1706.03741</em>.</p>
                <p>[14] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goswami, N., ... & Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. In <em>Advances in neural information processing systems</em> (pp. 9459-9474).</p>
            </div>
        </section>
    </main>

    <!-- Back to Top Button -->
    <button class="back-to-top" id="backToTop" aria-label="回到顶部">
        <i class="fas fa-arrow-up"></i>
    </button>

    <script>
        // Wrap everything in try-catch to prevent page crashes
        try {
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const themeIcon = document.getElementById('themeIcon');
        const html = document.documentElement;

        // Check for saved theme preference or default to light mode
        const currentTheme = localStorage.getItem('theme') || 'light';
        if (currentTheme === 'dark') {
            html.setAttribute('data-theme', 'dark');
            themeIcon.classList.remove('fa-moon');
            themeIcon.classList.add('fa-sun');
        }

        themeToggle.addEventListener('click', () => {
            const theme = html.getAttribute('data-theme');
            const newTheme = theme === 'dark' ? 'light' : 'dark';

            html.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);

            if (newTheme === 'dark') {
                themeIcon.classList.remove('fa-moon');
                themeIcon.classList.add('fa-sun');
            } else {
                themeIcon.classList.remove('fa-sun');
                themeIcon.classList.add('fa-moon');
            }

            // Re-render MathJax when theme changes
            setTimeout(() => {
                if (window.MathJax && window.MathJax.typesetPromise) {
                    window.MathJax.typesetPromise();
                }
            }, 300);
        });

        // Progress Bar
        const progressBar = document.getElementById('progressBar');

        window.addEventListener('scroll', () => {
            const scrollTop = window.scrollY;
            const docHeight = document.documentElement.scrollHeight - window.innerHeight;
            const scrollPercent = (scrollTop / docHeight) * 100;
            progressBar.style.width = scrollPercent + '%';
        });

    
        // Back to Top Button
        const backToTop = document.getElementById('backToTop');

        window.addEventListener('scroll', () => {
            if (window.scrollY > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        backToTop.addEventListener('click', () => {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Copy Code Function
        function copyCode(button) {
            const codeBlock = button.nextElementSibling;
            const code = codeBlock.textContent;

            navigator.clipboard.writeText(code).then(() => {
                const originalHTML = button.innerHTML;
                button.innerHTML = '<i class="fas fa-check mr-2"></i>已复制';
                button.style.backgroundColor = '#10b981';

                setTimeout(() => {
                    button.innerHTML = originalHTML;
                    button.style.backgroundColor = '';
                }, 2000);
            });
        }

        // Intersection Observer for fade-in animations
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        document.querySelectorAll('.content-section').forEach(section => {
            section.classList.add('animate-in');
            observer.observe(section);
        });

        // Initialize MathJax
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };

        
        // Prism.js initialization
        Prism.highlightAll();

        } catch (error) {
            console.error('JavaScript error in chapter3:', error);
            // Ensure content is visible even if JavaScript fails
            document.querySelectorAll('.content-section').forEach(section => {
                section.style.opacity = '1';
                section.style.transform = 'translateY(0)';
            });
        }
    </script>
</body>
</html>