<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第三章 大语言模型基础 - Hello Agents 2025</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script>
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    fontFamily: {
                        'sans': ['Inter', 'system-ui', 'sans-serif'],
                    },
                    colors: {
                        primary: {
                            50: '#f0f9ff',
                            100: '#e0f2fe',
                            500: '#0ea5e9',
                            600: '#0284c7',
                            700: '#0369a1',
                        }
                    }
                }
            }
        }
    </script>
    <style>
        /* 自定义样式 */
        .gradient-bg {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        
        .glass-effect {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        
        .text-gradient {
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        /* 深色模式样式 */
        .dark {
            background-color: #0f172a;
            color: #e2e8f0;
        }
        
        .dark .bg-white {
            background-color: #1e293b;
        }
        
        .dark .text-gray-800 {
            color: #e2e8f0;
        }
        
        .dark .text-gray-600 {
            color: #94a3b8;
        }
        
        .dark .border-gray-200 {
            border-color: #334155;
        }
        
        .dark .shadow-lg {
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.3), 0 4px 6px -2px rgba(0, 0, 0, 0.2);
        }

        .prose-custom {
            line-height: 1.75;
            max-width: none;
            color: #374151;
        }
        .dark .prose-custom {
            color: #e5e7eb;
        }
        .prose-custom h1 {
            font-size: 2.5rem;
            font-weight: 800;
            line-height: 1.2;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .prose-custom h2 {
            font-size: 2rem;
            font-weight: 700;
            line-height: 1.3;
            margin: 2.5rem 0 1.5rem 0;
            color: #1f2937;
            border-bottom: 2px solid #e5e7eb;
            padding-bottom: 0.5rem;
        }
        .dark .prose-custom h2 {
            color: #f9fafb;
            border-color: #374151;
        }
        .prose-custom h3 {
            font-size: 1.5rem;
            font-weight: 600;
            line-height: 1.4;
            margin: 2rem 0 1rem 0;
            color: #374151;
        }
        .dark .prose-custom h3 {
            color: #e5e7eb;
        }
        .prose-custom h4 {
            font-size: 1.25rem;
            font-weight: 600;
            line-height: 1.4;
            margin: 1.5rem 0 0.75rem 0;
            color: #4b5563;
        }
        .dark .prose-custom h4 {
            color: #d1d5db;
        }
        .code-block {
            background: #f8fafc;
            border: 1px solid #e2e8f0;
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 0.875rem;
            line-height: 1.6;
            overflow-x: auto;
        }
        .dark .code-block {
            background: #1e293b;
            border-color: #334155;
            color: #e2e8f0;
        }
        .image-container {
            margin: 2rem 0;
            text-align: center;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 0.75rem;
            box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .image-container img:hover {
            transform: scale(1.02);
            box-shadow: 0 20px 40px -10px rgba(0, 0, 0, 0.15);
        }
        .image-caption {
            margin-top: 1rem;
            font-size: 0.875rem;
            color: #6b7280;
            font-style: italic;
        }
        .dark .image-caption {
            color: #9ca3af;
        }
        .table-container {
            margin: 2rem 0;
            overflow-x: auto;
            border-radius: 0.75rem;
            border: 1px solid #e5e7eb;
        }
        .dark .table-container {
            border-color: #374151;
        }
        .table-container table {
            width: 100%;
            border-collapse: collapse;
        }
        .table-container th,
        .table-container td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid #e5e7eb;
        }
        .dark .table-container th,
        .dark .table-container td {
            border-color: #374151;
        }
        .table-container th {
            background: #f9fafb;
            font-weight: 600;
            color: #374151;
        }
        .dark .table-container th {
            background: #1f2937;
            color: #f9fafb;
        }
        .fade-in {
            animation: fadeIn 0.6s ease-in-out;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .hover-lift {
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }
        .hover-lift:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.1);
        }
        
        /* 滚动条样式 */
        ::-webkit-scrollbar {
            width: 8px;
        }
        ::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 4px;
        }
        .dark ::-webkit-scrollbar-track {
            background: #374151;
        }
        ::-webkit-scrollbar-thumb {
            background: #c1c1c1;
            border-radius: 4px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: #a8a8a8;
        }
        .dark ::-webkit-scrollbar-thumb {
            background: #6b7280;
        }
        .dark ::-webkit-scrollbar-thumb:hover {
            background: #9ca3af;
        }
        
        /* 返回顶部按钮 */
        .back-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 3rem;
            height: 3rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 50%;
            cursor: pointer;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            z-index: 1000;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
        }
        .back-to-top.show {
            opacity: 1;
            visibility: visible;
        }
        .back-to-top:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.2);
        }
        
        /* 特色卡片样式 */
        .feature-card {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
            border: 1px solid rgba(102, 126, 234, 0.2);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
            transition: all 0.3s ease;
        }
        .feature-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 20px 40px -10px rgba(102, 126, 234, 0.2);
        }
        .dark .feature-card {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(118, 75, 162, 0.05) 100%);
            border-color: rgba(102, 126, 234, 0.1);
        }

        /* 时间线样式 */
        .timeline {
            position: relative;
            padding-left: 2rem;
        }
        .timeline::before {
            content: '';
            position: absolute;
            left: 0.75rem;
            top: 0;
            bottom: 0;
            width: 2px;
            background: linear-gradient(to bottom, #667eea, #764ba2);
        }
        .timeline-item {
            position: relative;
            margin-bottom: 2rem;
            padding-left: 2rem;
        }
        .timeline-item::before {
            content: '';
            position: absolute;
            left: -0.5rem;
            top: 0.5rem;
            width: 1rem;
            height: 1rem;
            border-radius: 50%;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border: 3px solid white;
            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.2);
        }
        .dark .timeline-item::before {
            border-color: #1e293b;
        }

        /* 数学公式样式 */
        .math-container {
            background: #f8fafc;
            border: 1px solid #e2e8f0;
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            text-align: center;
        }
        .dark .math-container {
            background: #1e293b;
            border-color: #334155;
        }
        

    </style>
</head>
<body class="bg-white dark:bg-gray-900 text-gray-900 dark:text-gray-100 font-sans transition-colors duration-300">
    <!-- 导航栏 -->
    <nav class="bg-white dark:bg-gray-900 shadow-sm border-b border-gray-200 dark:border-gray-700 sticky top-0 z-50">
        <div class="max-w-7xl mx-auto px-6 py-4">
            <div class="flex items-center justify-between">
                <div class="flex items-center space-x-4">
                    <i class="fas fa-robot text-2xl text-gradient"></i>
                    <h1 class="text-xl font-bold text-gradient">Hello-Agents</h1>
                </div>
                <div class="flex items-center space-x-4">
                    <button id="theme-toggle" class="p-2 rounded-lg hover:bg-gray-100 dark:hover:bg-gray-800 transition-colors">
                        <i class="fas fa-moon text-gray-600 dark:text-gray-300"></i>
                    </button>
                </div>
            </div>
        </div>
    </nav>

    <!-- 返回顶部按钮 -->
    <button class="back-to-top" id="backToTop" title="返回顶部">
        <i class="fas fa-arrow-up"></i>
    </button>

    <!-- 主要内容 -->
    <main class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
        <article class="prose-custom fade-in">
            <!-- 章节标题 -->
            <header class="mb-12 text-center">
                <h1 class="mb-4">第三章 大语言模型基础</h1>
                <p class="text-lg text-gray-600 dark:text-gray-300 max-w-3xl mx-auto">
                    深入了解大语言模型的核心概念、技术原理和实际应用，为AI应用开发奠定坚实基础。从基础概念到前沿技术，全面掌握LLM的精髓。
                </p>
            </header>

            <!-- 引言 -->
            <section class="mb-12">
                <div class="feature-card">
                    <div class="flex items-center mb-4">
                        <i class="fas fa-lightbulb text-2xl text-gradient mr-4"></i>
                        <h3 class="text-xl font-semibold">学习目标</h3>
                    </div>
                    <ul class="space-y-2">
                        <li class="flex items-start">
                            <i class="fas fa-check-circle text-green-500 mr-2 mt-1"></i>
                            <span>理解大语言模型的基本概念和工作原理</span>
                        </li>
                        <li class="flex items-start">
                            <i class="fas fa-check-circle text-green-500 mr-2 mt-1"></i>
                            <span>掌握Transformer架构的核心机制</span>
                        </li>
                        <li class="flex items-start">
                            <i class="fas fa-check-circle text-green-500 mr-2 mt-1"></i>
                            <span>了解GPT系列模型的演进历程</span>
                        </li>
                        <li class="flex items-start">
                            <i class="fas fa-check-circle text-green-500 mr-2 mt-1"></i>
                            <span>学习提示工程的基本技巧</span>
                        </li>
                    </ul>
                </div>
            </section>

            <!-- 3.1 什么是大语言模型 -->
            <section class="mb-12">
                <h2 id="introduction">3.1 什么是大语言模型</h2>
                
                <p class="mb-6">
                    <strong>大语言模型（Large Language Model, LLM）</strong>是一种基于深度学习的人工智能模型，专门设计用于理解和生成人类语言。这些模型通过在海量文本数据上进行训练，学会了语言的模式、语法、语义，甚至是一定程度的常识和推理能力。
                </p>

                <div class="feature-card">
                    <h4 class="text-lg font-semibold mb-3 flex items-center">
                        <i class="fas fa-brain text-primary-500 mr-2"></i>
                        核心特征
                    </h4>
                    <ul class="space-y-2">
                        <li><strong>规模庞大</strong>：通常包含数十亿到数万亿个参数</li>
                        <li><strong>多任务能力</strong>：能够处理文本生成、翻译、问答、摘要等多种任务</li>
                        <li><strong>上下文理解</strong>：能够理解长文本的上下文关系</li>
                        <li><strong>零样本学习</strong>：无需特定训练即可处理新任务</li>
                    </ul>
                </div>

                <h3 id="development-history">3.1.1 语言模型的发展历程</h3>
                
                <div class="timeline">
                    <div class="timeline-item">
                        <h4 class="font-semibold text-primary-600 dark:text-primary-400">统计语言模型时代</h4>
                        <p>早期的语言模型主要基于统计方法，如N-gram模型。这些模型通过统计词汇的共现频率来预测下一个词，但受限于数据稀疏性和长距离依赖问题。</p>
                    </div>
                    
                    <div class="timeline-item">
                        <h4 class="font-semibold text-primary-600 dark:text-primary-400">神经网络语言模型</h4>
                        <p>随着深度学习的发展，RNN、LSTM等循环神经网络被应用于语言建模，能够更好地处理序列数据和长距离依赖。</p>
                    </div>
                    
                    <div class="timeline-item">
                        <h4 class="font-semibold text-primary-600 dark:text-primary-400">Transformer革命</h4>
                        <p>2017年，Transformer架构的提出彻底改变了自然语言处理领域，其自注意力机制使得模型能够并行处理序列，大大提升了训练效率。</p>
                    </div>
                    
                    <div class="timeline-item">
                        <h4 class="font-semibold text-primary-600 dark:text-primary-400">大语言模型时代</h4>
                        <p>基于Transformer的GPT、BERT等模型的出现，标志着大语言模型时代的到来。这些模型展现出了前所未有的语言理解和生成能力。</p>
                    </div>
                </div>

                <h3 id="working-principle">3.1.2 大语言模型的工作原理</h3>
                
                <p class="mb-4">
                    大语言模型的核心工作原理可以概括为<strong>"下一个词预测"</strong>。给定一个文本序列，模型会预测最可能出现的下一个词。这个看似简单的任务，实际上需要模型理解语法、语义、上下文关系，甚至是世界知识。
                </p>

                <div class="image-container">
                    <img src="../docs/images/3-figures/1757249275674-0.png" alt="语言模型预测示例" class="mx-auto">
                    <p class="image-caption">图3.1 语言模型的基本工作原理：根据前文预测下一个词</p>
                </div>

                <div class="code-block">
                    <h4 class="text-sm font-semibold mb-2 text-gray-600 dark:text-gray-300">示例：简单的语言模型预测</h4>
                    <pre><code>输入：今天天气很
可能的预测：
- 好 (概率: 0.4)
- 热 (概率: 0.2)  
- 冷 (概率: 0.15)
- 晴朗 (概率: 0.1)
- 其他 (概率: 0.15)</code></pre>
                </div>

                <h3 id="llm-to-assistant-intro">3.1.3 从语言模型到智能助手</h3>
                
                <p class="mb-4">
                    现代的大语言模型不仅仅是语言模型，它们经过了复杂的训练过程，包括：
                </p>

                <div class="grid md:grid-cols-3 gap-6 mb-6">
                    <div class="feature-card">
                        <div class="text-center">
                            <i class="fas fa-database text-3xl text-primary-500 mb-3"></i>
                            <h4 class="font-semibold mb-2">预训练</h4>
                            <p class="text-sm">在海量文本数据上进行无监督学习，学习语言的基本模式</p>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <div class="text-center">
                            <i class="fas fa-cogs text-3xl text-primary-500 mb-3"></i>
                            <h4 class="font-semibold mb-2">指令调优</h4>
                            <p class="text-sm">使用指令-回答对进行监督微调，提升指令遵循能力</p>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <div class="text-center">
                            <i class="fas fa-thumbs-up text-3xl text-primary-500 mb-3"></i>
                            <h4 class="font-semibold mb-2">人类反馈</h4>
                            <p class="text-sm">通过人类反馈的强化学习，使模型输出更符合人类偏好</p>
                        </div>
                    </div>
                </div>
            </section>
                
            <!-- 3.2 大语言模型的核心技术 -->
            <section class="mb-12">
                <h2 id="core-tech">3.2 大语言模型的核心技术</h2>
                
                <p class="mb-6">
                    大语言模型的强大能力源于多项核心技术的结合。本节将深入探讨这些关键技术，包括Transformer架构、提示工程、文本分词等。
                </p>

                <h3 id="prompt-engineering">3.2.1 提示工程</h3>
                
                <p class="mb-4">
                    <strong>提示工程（Prompt Engineering）</strong>是与大语言模型交互的艺术和科学。一个精心设计的提示可以显著提升模型的表现，而糟糕的提示则可能导致模型产生无关或错误的输出。
                </p>

                <div class="feature-card">
                    <h4 class="text-lg font-semibold mb-4 flex items-center">
                        <i class="fas fa-magic text-primary-500 mr-2"></i>
                        模型采样参数
                    </h4>
                    
                    <div class="grid md:grid-cols-3 gap-6 mb-6">
                        <div class="bg-white dark:bg-gray-800 rounded-lg p-4 border border-gray-200 dark:border-gray-700">
                            <h5 class="font-semibold text-blue-600 mb-2">Temperature</h5>
                            <p class="text-sm mb-3">控制输出的随机性和创造性</p>
                            <div class="space-y-2 text-xs">
                                <div class="flex justify-between">
                                    <span>0.0</span>
                                    <span class="text-gray-500">确定性输出</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>0.7</span>
                                    <span class="text-gray-500">平衡</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>1.0+</span>
                                    <span class="text-gray-500">高创造性</span>
                                </div>
                            </div>
                        </div>
                        
                        <div class="bg-white dark:bg-gray-800 rounded-lg p-4 border border-gray-200 dark:border-gray-700">
                            <h5 class="font-semibold text-green-600 mb-2">Top-k</h5>
                            <p class="text-sm mb-3">限制候选词的数量</p>
                            <div class="space-y-2 text-xs">
                                <div class="flex justify-between">
                                    <span>1</span>
                                    <span class="text-gray-500">贪心选择</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>40</span>
                                    <span class="text-gray-500">常用设置</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>∞</span>
                                    <span class="text-gray-500">无限制</span>
                                </div>
                            </div>
                        </div>
                        
                        <div class="bg-white dark:bg-gray-800 rounded-lg p-4 border border-gray-200 dark:border-gray-700">
                            <h5 class="font-semibold text-purple-600 mb-2">Top-p</h5>
                            <p class="text-sm mb-3">基于累积概率的动态筛选</p>
                            <div class="space-y-2 text-xs">
                                <div class="flex justify-between">
                                    <span>0.1</span>
                                    <span class="text-gray-500">保守</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>0.9</span>
                                    <span class="text-gray-500">常用设置</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>1.0</span>
                                    <span class="text-gray-500">无筛选</span>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="bg-yellow-50 dark:bg-yellow-900/20 border-l-4 border-yellow-500 p-4 rounded-r-lg">
                        <p class="text-sm">
                            <strong>参数协同工作：</strong>温度调整→Top-k→Top-p 的分层过滤方式。通常 Top-k 和 Top-p 二选一即可。
                        </p>
                    </div>
                </div>

                <h4>零样本、单样本与少样本提示</h4>
                
                <div class="grid md:grid-cols-3 gap-6 mb-6">
                    <div class="feature-card">
                        <h5 class="font-semibold text-blue-600 mb-3">零样本提示</h5>
                        <p class="text-sm mb-3">直接给出指令，不提供示例</p>
                        <div class="code-block">
                            <pre><code>文本：Datawhale的AI Agent课程非常棒！
情感：正面</code></pre>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold text-green-600 mb-3">单样本提示</h5>
                        <p class="text-sm mb-3">提供一个完整示例</p>
                        <div class="code-block">
                            <pre><code>文本：这家餐厅的服务太慢了。
情感：负面

文本：Datawhale的AI Agent课程非常棒！
情感：</code></pre>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold text-purple-600 mb-3">少样本提示</h5>
                        <p class="text-sm mb-3">提供多个示例，覆盖不同情况</p>
                        <div class="code-block">
                            <pre><code>文本：这家餐厅的服务太慢了。
情感：负面

文本：这部电影的情节很平淡。
情感：中性

文本：Datawhale的AI Agent课程非常棒！
情感：</code></pre>
                        </div>
                    </div>
                </div>

                <h4>基础提示技巧</h4>
                
                <div class="grid md:grid-cols-2 gap-6 mb-6">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 flex items-center">
                            <i class="fas fa-user-tie text-blue-500 mr-2"></i>
                            角色扮演
                        </h5>
                        <p class="text-sm mb-3">通过赋予模型特定角色，引导其回答风格和知识范围</p>
                        <div class="code-block">
                            <pre><code>你现在是一位资深的Python编程专家。请解释一下Python中的GIL（全局解释器锁）是什么，要让一个初学者也能听懂。</code></pre>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 flex items-center">
                            <i class="fas fa-list-alt text-green-500 mr-2"></i>
                            上下文示例
                        </h5>
                        <p class="text-sm mb-3">提供清晰的输入输出示例，教会模型处理特定格式</p>
                        <div class="code-block">
                            <pre><code>评论：这款"星尘"笔记本电脑的屏幕显示效果惊人，但我不太喜欢它的键盘手感。
输出：{"product_name": "星尘笔记本电脑", "sentiment": "混合"}

评论：我刚买的"声动"耳机音质很棒，续航也超出了我的预期！
输出：</code></pre>
                        </div>
                    </div>
                </div>

                <h4>思维链（Chain-of-Thought）</h4>
                
                <p class="mb-4">
                    对于需要逻辑推理、计算或多步骤思考的复杂问题，<strong>思维链</strong>通过引导模型"一步一步地思考"，显著提升推理能力。
                </p>

                <div class="feature-card">
                    <h5 class="font-semibold mb-3 flex items-center">
                        <i class="fas fa-brain text-purple-500 mr-2"></i>
                        思维链示例
                    </h5>
                    <div class="code-block">
                        <pre><code>一个篮球队在一个赛季的80场比赛中赢了60%。在接下来的赛季中，他们打了15场比赛，赢了12场。两个赛季的总胜率是多少？
请一步一步地思考并解答。

>>> 
好的，我们来一步步计算。
第一步：计算第一个赛季赢得的比赛数。
80场 * 60% = 48场。
第二步：计算两个赛季的总比赛数和总胜利数。
总比赛数 = 80 + 15 = 95场。
总胜利数 = 48 + 12 = 60场。
第三步：计算总胜率。
总胜率 = (总胜利数 / 总比赛数) * 100% = (60 / 95) * 100% ≈ 63.16%。
所以，两个赛季的总胜率约为63.16%。</code></pre>
                    </div>
                </div>

                <h3 id="tokenization">3.2.2 文本分词</h3>
                
                <p class="mb-4">
                    计算机本质上只能理解数字，因此需要将自然语言文本转换成数字格式。<strong>分词（Tokenization）</strong>就是将文本序列转换为数字序列的过程。
                </p>

                <div class="image-container">
                    <img src="../docs/images/3-figures/1757249275674-1.png" alt="分词过程示例" class="mx-auto">
                    <p class="image-caption">图3.2 文本分词的基本流程</p>
                </div>

                <h4>为何需要分词</h4>
                
                <div class="grid md:grid-cols-3 gap-6 mb-6">
                    <div class="feature-card">
                        <h5 class="font-semibold text-red-600 mb-3">按词分词</h5>
                        <p class="text-sm mb-3">用空格或标点符号切分</p>
                        <div class="space-y-2 text-xs">
                            <div class="flex items-center">
                                <i class="fas fa-check text-green-500 mr-2"></i>
                                <span>直观易懂</span>
                            </div>
                            <div class="flex items-center">
                                <i class="fas fa-times text-red-500 mr-2"></i>
                                <span>词表爆炸</span>
                            </div>
                            <div class="flex items-center">
                                <i class="fas fa-times text-red-500 mr-2"></i>
                                <span>OOV问题</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold text-blue-600 mb-3">按字符分词</h5>
                        <p class="text-sm mb-3">切分成单个字符</p>
                        <div class="space-y-2 text-xs">
                            <div class="flex items-center">
                                <i class="fas fa-check text-green-500 mr-2"></i>
                                <span>词表很小</span>
                            </div>
                            <div class="flex items-center">
                                <i class="fas fa-check text-green-500 mr-2"></i>
                                <span>无OOV问题</span>
                            </div>
                            <div class="flex items-center">
                                <i class="fas fa-times text-red-500 mr-2"></i>
                                <span>语义缺失</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold text-green-600 mb-3">子词分词</h5>
                        <p class="text-sm mb-3">平衡词表大小和语义</p>
                        <div class="space-y-2 text-xs">
                            <div class="flex items-center">
                                <i class="fas fa-check text-green-500 mr-2"></i>
                                <span>词表可控</span>
                            </div>
                            <div class="flex items-center">
                                <i class="fas fa-check text-green-500 mr-2"></i>
                                <span>保留语义</span>
                            </div>
                            <div class="flex items-center">
                                <i class="fas fa-check text-green-500 mr-2"></i>
                                <span>处理新词</span>
                            </div>
                        </div>
                    </div>
                </div>

                <h4>字节对编码算法（BPE）</h4>
                
                <p class="mb-4">
                    <strong>字节对编码（Byte-Pair Encoding, BPE）</strong>是最主流的子词分词算法，GPT系列模型采用此算法。其核心思想是"贪心"的合并过程。
                </p>

                <div class="feature-card">
                    <h5 class="font-semibold mb-4">BPE算法步骤</h5>
                    <div class="grid md:grid-cols-3 gap-4 mb-4">
                        <div class="text-center p-4 bg-blue-50 dark:bg-blue-900/20 rounded-lg">
                            <div class="w-12 h-12 bg-blue-500 text-white rounded-full flex items-center justify-center mx-auto mb-2">1</div>
                            <h6 class="font-semibold">初始化</h6>
                            <p class="text-xs">将词表初始化为所有基本字符</p>
                        </div>
                        <div class="text-center p-4 bg-green-50 dark:bg-green-900/20 rounded-lg">
                            <div class="w-12 h-12 bg-green-500 text-white rounded-full flex items-center justify-center mx-auto mb-2">2</div>
                            <h6 class="font-semibold">迭代合并</h6>
                            <p class="text-xs">统计频率最高的词元对并合并</p>
                        </div>
                        <div class="text-center p-4 bg-purple-50 dark:bg-purple-900/20 rounded-lg">
                            <div class="w-12 h-12 bg-purple-500 text-white rounded-full flex items-center justify-center mx-auto mb-2">3</div>
                            <h6 class="font-semibold">重复</h6>
                            <p class="text-xs">直到词表达到预设大小</p>
                        </div>
                    </div>
                </div>

                <div class="image-container">
                    <img src="../docs/images/3-figures/1757249275674-5.png" alt="BPE算法合并过程示例" class="mx-auto">
                    <p class="image-caption">表3.1 BPE算法合并过程示例</p>
                </div>

                <div class="code-block">
                    <h5 class="text-sm font-semibold mb-2 text-gray-600 dark:text-gray-300">BPE算法Python实现示例</h5>
                    <pre><code>import re, collections

def get_stats(vocab):
    """统计词元对频率"""
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    """合并词元对"""
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

# 准备语料库
vocab = {'h u g </w>': 1, 'p u g </w>': 1, 'p u n </w>': 1, 'b u n </w>': 1}
num_merges = 4

for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(f"第{i+1}次合并: {best} -> {''.join(best)}")
    print(f"新词表（部分）: {list(vocab.keys())}")
    print("-" * 20)</code></pre>
                </div>

                <h4>分词器的实际意义</h4>
                
                <div class="grid md:grid-cols-3 gap-6 mb-6">
                    <div class="feature-card">
                        <div class="text-center">
                            <i class="fas fa-window-maximize text-3xl text-blue-500 mb-3"></i>
                            <h5 class="font-semibold mb-2">上下文窗口</h5>
                            <p class="text-sm">模型的上下文窗口以Token数量计算，不是字符数</p>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <div class="text-center">
                            <i class="fas fa-dollar-sign text-3xl text-green-500 mb-3"></i>
                            <h5 class="font-semibold mb-2">API成本</h5>
                            <p class="text-sm">大多数模型API按Token数量计费，了解分词有助于成本控制</p>
                        </div>
                    </div>
                    
                    <div class="feature-card">
                        <div class="text-center">
                            <i class="fas fa-bug text-3xl text-red-500 mb-3"></i>
                            <h5 class="font-semibold mb-2">异常表现</h5>
                            <p class="text-sm">模型的奇怪表现可能源于分词，如空格、大小写的影响</p>
                        </div>
                    </div>
                </div>

                <h3 id="open-source-llm">3.2.3 调用开源大语言模型</h3>
                
                <p class="mb-4">
                    除了通过API调用模型，我们也可以直接在本地部署开源大语言模型。<strong>Hugging Face Transformers</strong>提供了标准化的接口来加载和使用预训练模型。
                </p>

                <div class="feature-card">
                    <h5 class="font-semibold mb-4 flex items-center">
                        <i class="fas fa-download text-blue-500 mr-2"></i>
                        环境配置与模型选择
                    </h5>
                    <p class="text-sm mb-4">我们选择<code>Qwen/Qwen1.5-0.5B-Chat</code>模型进行演示，这是一个轻量级但功能强大的对话模型。</p>
                    
                    <div class="code-block">
                        <pre><code>pip install transformers torch</code></pre>
                    </div>
                </div>

                <div class="code-block">
                    <h5 class="text-sm font-semibold mb-2 text-gray-600 dark:text-gray-300">加载模型和分词器</h5>
                    <pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# 指定模型ID
model_id = "Qwen/Qwen1.5-0.5B-Chat"

# 设置设备，优先使用GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 加载分词器
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 加载模型，并将其移动到指定设备
model = AutoModelForCausalLM.from_pretrained(model_id).to(device)

print("模型和分词器加载完成！")</code></pre>
                </div>

                <div class="code-block">
                    <h5 class="text-sm font-semibold mb-2 text-gray-600 dark:text-gray-300">准备对话输入并生成回答</h5>
                    <pre><code># 准备对话输入
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "你好，请介绍你自己。"}
]

# 使用分词器的模板格式化输入
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# 编码输入文本
model_inputs = tokenizer([text], return_tensors="pt").to(device)

# 使用模型生成回答
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)

# 解码生成的Token ID
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print("\n模型的回答:")
print(response)</code></pre>
                </div>

                <h3 id="model-selection">3.2.4 模型的选择</h3>
                
                <p class="mb-4">
                    在众多模型中选择最适合的并非简单地追求"最大、最强"，而是在性能、成本、速度和部署方式之间进行权衡的决策过程。
                </p>

                <h4>模型选型的关键考量</h4>
                
                <div class="grid md:grid-cols-2 gap-6 mb-6">
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 flex items-center">
                            <i class="fas fa-chart-line text-blue-500 mr-2"></i>
                            性能与能力
                        </h5>
                        <ul class="space-y-2 text-sm">
                            <li>• 不同模型擅长不同任务</li>
                            <li>• 参考公开基准测试排行榜</li>
                            <li>• 考虑逻辑推理、代码生成等具体能力</li>
                            <li>• 评估多语言支持情况</li>
                        </ul>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 flex items-center">
                            <i class="fas fa-dollar-sign text-green-500 mr-2"></i>
                            成本考量
                        </h5>
                        <ul class="space-y-2 text-sm">
                            <li>• 闭源模型：API调用费用（按Token计费）</li>
                            <li>• 开源模型：硬件成本（GPU、内存）</li>
                            <li>• 运维成本和技术门槛</li>
                            <li>• 预期使用量和预算匹配</li>
                        </ul>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 flex items-center">
                            <i class="fas fa-tachometer-alt text-purple-500 mr-2"></i>
                            速度与延迟
                        </h5>
                        <ul class="space-y-2 text-sm">
                            <li>• 实时交互应用对延迟敏感</li>
                            <li>• 轻量级模型响应更快</li>
                            <li>• 考虑网络延迟和服务稳定性</li>
                            <li>• 批处理vs实时处理需求</li>
                        </ul>
                    </div>
                    
                    <div class="feature-card">
                        <h5 class="font-semibold mb-3 flex items-center">
                            <i class="fas fa-window-maximize text-orange-500 mr-2"></i>
                            上下文窗口
                        </h5>
                        <ul class="space-y-2 text-sm">
                            <li>• 长文档分析需要大窗口</li>
                            <li>• 代码库理解和长期对话记忆</li>
                            <li>• 128K Token或更高的选择</li>
                            <li>• 平衡窗口大小和处理速度</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- 3.3 Transformer架构详解 -->
            <section class="mb-12">
                <h2 id="transformer">3.3 Transformer架构详解</h2>
                
                <p class="text-lg leading-relaxed mb-8">
                    Transformer是现代大语言模型的核心架构，它完全基于注意力机制，摒弃了传统的循环和卷积结构。本节将深入解析Transformer的各个组件。
                </p>

                <div class="image-container">
                    <img src="../docs/images/3-figures/1757249275674-2.png" alt="Transformer架构图" class="mx-auto">
                    <p class="image-caption">图3.3 Transformer整体架构</p>
                </div>

                <h3>3.3.1 Encoder-Decoder整体结构</h3>
                
                <p class="mb-6">
                    原始的Transformer采用编码器-解码器（Encoder-Decoder）架构，编码器负责理解输入序列，解码器负责生成输出序列。
                </p>

                <div class="feature-card">
                    <h4 class="text-lg font-semibold mb-6 text-center flex items-center justify-center">
                        <i class="fas fa-sitemap text-gradient mr-2"></i>
                        Transformer整体架构
                    </h4>
                    <div class="grid md:grid-cols-2 gap-8">
                        <!-- 编码器 -->
                        <div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg hover-lift">
                            <h4 class="text-lg font-semibold mb-4 text-blue-600 flex items-center">
                                <i class="fas fa-arrow-right mr-2"></i>
                                编码器（Encoder）
                            </h4>
                            <ul class="space-y-2 text-sm">
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-blue-500 mr-2 mt-1"></i>
                                    <span>多头自注意力机制</span>
                                </li>
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-blue-500 mr-2 mt-1"></i>
                                    <span>前馈神经网络</span>
                                </li>
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-blue-500 mr-2 mt-1"></i>
                                    <span>残差连接和层归一化</span>
                                </li>
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-blue-500 mr-2 mt-1"></i>
                                    <span>位置编码</span>
                                </li>
                            </ul>
                        </div>

                        <!-- 解码器 -->
                        <div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg hover-lift">
                            <h4 class="text-lg font-semibold mb-4 text-green-600 flex items-center">
                                <i class="fas fa-arrow-left mr-2"></i>
                                解码器（Decoder）
                            </h4>
                            <ul class="space-y-2 text-sm">
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-green-500 mr-2 mt-1"></i>
                                    <span>掩码多头自注意力</span>
                                </li>
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-green-500 mr-2 mt-1"></i>
                                    <span>编码器-解码器注意力</span>
                                </li>
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-green-500 mr-2 mt-1"></i>
                                    <span>前馈神经网络</span>
                                </li>
                                <li class="flex items-start">
                                    <i class="fas fa-dot-circle text-green-500 mr-2 mt-1"></i>
                                    <span>输出层和Softmax</span>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="code-block">
                    <h5 class="text-sm font-semibold mb-2 text-gray-600 dark:text-gray-300">Transformer编码器层的PyTorch实现</h5>
                    <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)
        return output, attention_weights
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 线性变换并重塑为多头
        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 应用注意力
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # 连接多头并通过输出投影
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        output = self.W_o(attention_output)
        
        return output

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # 自注意力 + 残差连接 + 层归一化
        attention_output = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attention_output))
        
        # 前馈网络 + 残差连接 + 层归一化
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x</code></pre>
                </div>

                <h3>3.3.2 自注意力机制</h3>
                 
                 <p class="leading-relaxed mb-6">
                     自注意力机制是Transformer的核心创新，它允许模型在处理序列中的每个位置时，都能够关注到序列中的所有其他位置。这种机制使得模型能够捕捉长距离依赖关系，并且可以并行计算。
                 </p>

                 <div class="image-container">
                     <img src="../docs/images/3-figures/1757249275674-3.png" alt="自注意力机制示意图" class="mx-auto">
                     <p class="image-caption">图3.4 自注意力机制的计算过程</p>
                 </div>

                 <h4>注意力计算公式</h4>
                 
                 <div class="math-container">
                     <p class="text-lg font-semibold mb-4">缩放点积注意力（Scaled Dot-Product Attention）</p>
                     <div class="text-xl font-mono">
                         Attention(Q, K, V) = softmax(QK<sup>T</sup> / √d<sub>k</sub>)V
                     </div>
                     <div class="mt-4 text-sm text-gray-600 dark:text-gray-400">
                         <p>其中：Q = 查询矩阵，K = 键矩阵，V = 值矩阵，d<sub>k</sub> = 键向量的维度</p>
                     </div>
                 </div>

                 <div class="grid md:grid-cols-3 gap-6 mb-6">
                     <div class="feature-card">
                         <h5 class="font-semibold text-blue-600 mb-3 flex items-center">
                             <i class="fas fa-search text-blue-500 mr-2"></i>
                             查询（Query）
                         </h5>
                         <p class="text-sm">当前位置的表示，用于与其他位置进行匹配</p>
                     </div>
                     
                     <div class="feature-card">
                         <h5 class="font-semibold text-green-600 mb-3 flex items-center">
                             <i class="fas fa-key text-green-500 mr-2"></i>
                             键（Key）
                         </h5>
                         <p class="text-sm">序列中每个位置的标识，用于计算注意力权重</p>
                     </div>
                     
                     <div class="feature-card">
                         <h5 class="font-semibold text-purple-600 mb-3 flex items-center">
                             <i class="fas fa-gem text-purple-500 mr-2"></i>
                             值（Value）
                         </h5>
                         <p class="text-sm">实际的信息内容，根据注意力权重进行加权求和</p>
                     </div>
                 </div>

                 <h4>多头注意力机制</h4>
                 
                 <p class="mb-4">
                     多头注意力允许模型在不同的表示子空间中并行地关注信息，每个"头"都学习不同类型的依赖关系。
                 </p>

                 <div class="image-container">
                     <img src="../docs/images/3-figures/1757249275674-4.png" alt="多头注意力机制" class="mx-auto">
                     <p class="image-caption">图3.5 多头注意力机制的并行计算</p>
                 </div>

                 <h3>3.3.3 位置编码</h3>
                 
                 <p class="mb-4">
                     由于Transformer没有循环或卷积结构，它无法感知序列中元素的位置信息。位置编码为模型提供了关于token在序列中位置的信息。
                 </p>

                 <div class="feature-card">
                     <h5 class="font-semibold mb-4">正弦位置编码公式</h5>
                     <div class="math-container">
                         <div class="grid md:grid-cols-2 gap-4 text-center">
                             <div>
                                 <p class="font-mono text-lg">PE(pos, 2i) = sin(pos / 10000<sup>2i/d</sup>)</p>
                                 <p class="text-sm text-gray-600 dark:text-gray-400 mt-2">偶数维度使用正弦函数</p>
                             </div>
                             <div>
                                 <p class="font-mono text-lg">PE(pos, 2i+1) = cos(pos / 10000<sup>2i/d</sup>)</p>
                                 <p class="text-sm text-gray-600 dark:text-gray-400 mt-2">奇数维度使用余弦函数</p>
                             </div>
                         </div>
                     </div>
                 </div>

                 <h3>3.3.4 前馈神经网络</h3>
                 
                 <p class="mb-4">
                     每个Transformer层都包含一个位置无关的前馈神经网络，它对每个位置的表示进行相同的变换。
                 </p>

                 <div class="code-block">
                     <h5 class="text-sm font-semibold mb-2 text-gray-600 dark:text-gray-300">前馈网络的实现</h5>
                     <pre><code>class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))</code></pre>
                 </div>
             </section>

             <!-- 3.4 GPT系列模型演进 -->
             <section class="mb-12">
                 <h2 id="gpt-evolution">3.4 GPT系列模型演进</h2>
                 
                 <p class="text-lg leading-relaxed mb-8">
                     GPT（Generative Pre-trained Transformer）系列模型代表了大语言模型发展的重要里程碑。从GPT-1到GPT-4，每一代模型都在规模、能力和应用范围上实现了显著突破。
                 </p>

                 <div class="timeline">
                     <div class="timeline-item">
                         <h4 class="font-semibold text-primary-600 dark:text-primary-400 mb-2">GPT-1 (2018)</h4>
                         <div class="grid md:grid-cols-2 gap-4">
                             <div>
                                 <p class="text-sm mb-2"><strong>参数规模：</strong>1.17亿参数</p>
                                 <p class="text-sm mb-2"><strong>训练数据：</strong>BooksCorpus数据集</p>
                                 <p class="text-sm"><strong>创新点：</strong>证明了无监督预训练+有监督微调的有效性</p>
                             </div>
                             <div class="text-sm">
                                 <p>GPT-1首次展示了大规模预训练语言模型的潜力，在多个NLP任务上取得了当时的最佳性能。</p>
                             </div>
                         </div>
                     </div>
                     
                     <div class="timeline-item">
                         <h4 class="font-semibold text-primary-600 dark:text-primary-400 mb-2">GPT-2 (2019)</h4>
                         <div class="grid md:grid-cols-2 gap-4">
                             <div>
                                 <p class="text-sm mb-2"><strong>参数规模：</strong>15亿参数</p>
                                 <p class="text-sm mb-2"><strong>训练数据：</strong>WebText（40GB文本）</p>
                                 <p class="text-sm"><strong>创新点：</strong>零样本任务执行能力</p>
                             </div>
                             <div class="text-sm">
                                 <p>GPT-2展现了强大的文本生成能力，OpenAI最初因担心滥用而延迟发布完整模型。</p>
                             </div>
                         </div>
                     </div>
                     
                     <div class="timeline-item">
                         <h4 class="font-semibold text-primary-600 dark:text-primary-400 mb-2">GPT-3 (2020)</h4>
                         <div class="grid md:grid-cols-2 gap-4">
                             <div>
                                 <p class="text-sm mb-2"><strong>参数规模：</strong>1750亿参数</p>
                                 <p class="text-sm mb-2"><strong>训练数据：</strong>Common Crawl等（570GB）</p>
                                 <p class="text-sm"><strong>创新点：</strong>涌现能力，少样本学习</p>
                             </div>
                             <div class="text-sm">
                                 <p>GPT-3标志着大语言模型进入新时代，展现了令人惊叹的通用智能，催生了AI应用的爆发。</p>
                             </div>
                         </div>
                     </div>
                     
                     <div class="timeline-item">
                         <h4 class="font-semibold text-primary-600 dark:text-primary-400 mb-2">GPT-4 (2023)</h4>
                         <div class="grid md:grid-cols-2 gap-4">
                             <div>
                                 <p class="text-sm mb-2"><strong>参数规模：</strong>未公开（估计万亿级）</p>
                                 <p class="text-sm mb-2"><strong>训练数据：</strong>多模态数据</p>
                                 <p class="text-sm"><strong>创新点：</strong>多模态能力，更强推理</p>
                             </div>
                             <div class="text-sm">
                                 <p>GPT-4实现了文本和图像的多模态理解，在专业考试中达到人类专家水平。</p>
                             </div>
                         </div>
                     </div>
                 </div>

                 <h3>3.4.1 涌现能力</h3>
                 
                 <p class="mb-4">
                     随着模型规模的增长，大语言模型展现出了一些在小模型中不存在的<strong>涌现能力（Emergent Abilities）</strong>。这些能力似乎在达到某个临界规模后突然出现。
                 </p>

                 <div class="grid md:grid-cols-3 gap-6 mb-6">
                     <div class="feature-card">
                         <div class="text-center">
                             <i class="fas fa-lightbulb text-3xl text-yellow-500 mb-3"></i>
                             <h5 class="font-semibold mb-2">少样本学习</h5>
                             <p class="text-sm">仅通过几个示例就能学会新任务</p>
                         </div>
                     </div>
                     
                     <div class="feature-card">
                         <div class="text-center">
                             <i class="fas fa-brain text-3xl text-purple-500 mb-3"></i>
                             <h5 class="font-semibold mb-2">复杂推理</h5>
                             <p class="text-sm">能够进行多步骤的逻辑推理和数学计算</p>
                         </div>
                     </div>
                     
                     <div class="feature-card">
                         <div class="text-center">
                             <i class="fas fa-code text-3xl text-green-500 mb-3"></i>
                             <h5 class="font-semibold mb-2">代码生成</h5>
                             <p class="text-sm">理解需求并生成高质量的程序代码</p>
                         </div>
                     </div>
                 </div>
             </section>

             <!-- 3.5 实践练习 -->
             <section class="mb-12">
                 <h2 id="practice">3.5 实践练习</h2>
                 
                 <p class="mb-6">
                     通过实际操作来加深对大语言模型的理解。以下练习将帮助你掌握提示工程的基本技巧。
                 </p>

                 <div class="feature-card">
                     <h4 class="text-lg font-semibold mb-4 flex items-center">
                         <i class="fas fa-tasks text-primary-500 mr-2"></i>
                         练习任务
                     </h4>
                     
                     <div class="space-y-6">
                         <div class="border-l-4 border-blue-500 pl-4">
                             <h5 class="font-semibold text-blue-600 mb-2">任务1：提示优化</h5>
                             <p class="text-sm mb-2">尝试不同的提示方式，观察模型输出的变化：</p>
                             <ul class="text-sm space-y-1 ml-4">
                                 <li>• 简单指令 vs 详细指令</li>
                                 <li>• 零样本 vs 少样本提示</li>
                                 <li>• 不同的角色设定</li>
                             </ul>
                         </div>
                         
                         <div class="border-l-4 border-green-500 pl-4">
                             <h5 class="font-semibold text-green-600 mb-2">任务2：思维链推理</h5>
                             <p class="text-sm mb-2">使用思维链提示解决复杂问题：</p>
                             <ul class="text-sm space-y-1 ml-4">
                                 <li>• 数学应用题</li>
                                 <li>• 逻辑推理问题</li>
                                 <li>• 多步骤分析任务</li>
                             </ul>
                         </div>
                         
                         <div class="border-l-4 border-purple-500 pl-4">
                             <h5 class="font-semibold text-purple-600 mb-2">任务3：参数调优</h5>
                             <p class="text-sm mb-2">实验不同的生成参数：</p>
                             <ul class="text-sm space-y-1 ml-4">
                                 <li>• 调整temperature观察创造性变化</li>
                                 <li>• 比较top-k和top-p的效果</li>
                                 <li>• 测试不同max_tokens的影响</li>
                             </ul>
                         </div>
                     </div>
                 </div>
             </section>

             <!-- 章节总结 -->
             <section class="mb-12">
                 <h2 id="summary">3.6 本章总结</h2>
                 
                 <div class="feature-card">
                     <h4 class="text-lg font-semibold mb-4 flex items-center">
                         <i class="fas fa-check-circle text-green-500 mr-2"></i>
                         关键要点回顾
                     </h4>
                     
                     <div class="grid md:grid-cols-2 gap-6">
                         <div>
                             <h5 class="font-semibold mb-3">核心概念</h5>
                             <ul class="space-y-2 text-sm">
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-blue-500 mr-2 mt-1"></i>
                                     <span>大语言模型的工作原理和特征</span>
                                 </li>
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-blue-500 mr-2 mt-1"></i>
                                     <span>Transformer架构的核心组件</span>
                                 </li>
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-blue-500 mr-2 mt-1"></i>
                                     <span>自注意力机制的计算过程</span>
                                 </li>
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-blue-500 mr-2 mt-1"></i>
                                     <span>文本分词和BPE算法</span>
                                 </li>
                             </ul>
                         </div>
                         
                         <div>
                             <h5 class="font-semibold mb-3">实用技能</h5>
                             <ul class="space-y-2 text-sm">
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-green-500 mr-2 mt-1"></i>
                                     <span>提示工程的基本技巧</span>
                                 </li>
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-green-500 mr-2 mt-1"></i>
                                     <span>模型参数的调优方法</span>
                                 </li>
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-green-500 mr-2 mt-1"></i>
                                     <span>开源模型的部署和使用</span>
                                 </li>
                                 <li class="flex items-start">
                                     <i class="fas fa-arrow-right text-green-500 mr-2 mt-1"></i>
                                     <span>模型选择的考量因素</span>
                                 </li>
                             </ul>
                         </div>
                     </div>
                 </div>

                 <div class="bg-gradient-to-r from-blue-50 to-purple-50 dark:from-blue-900/20 dark:to-purple-900/20 rounded-lg p-6 mt-8">
                     <h4 class="text-lg font-semibold mb-3 text-center">下一步学习建议</h4>
                     <p class="text-center text-gray-700 dark:text-gray-300">
                         掌握了大语言模型的基础知识后，接下来我们将学习如何构建智能代理（Agent），
                         让模型不仅能够理解和生成文本，还能够执行复杂的任务和与外部环境交互。
                     </p>
                 </div>
             </section>
         </article>
     </main>

     <script>
         // 主题切换功能
         class ThemeManager {
             constructor() {
                 this.themeToggle = document.getElementById('theme-toggle');
                 this.init();
             }

             init() {
                 // 检查系统主题偏好
                 const savedTheme = localStorage.getItem('theme');
                 const systemPrefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
                 
                 if (savedTheme) {
                     this.setTheme(savedTheme);
                 } else if (systemPrefersDark) {
                     this.setTheme('dark');
                 }

                 this.bindEvents();
             }

             bindEvents() {
                 if (this.themeToggle) {
                     this.themeToggle.addEventListener('click', () => this.toggleTheme());
                 }

                 // 监听系统主题变化
                 window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', (e) => {
                     if (!localStorage.getItem('theme')) {
                         this.setTheme(e.matches ? 'dark' : 'light');
                     }
                 });
             }

             toggleTheme() {
                 const currentTheme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                 const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
                 this.setTheme(newTheme);
                 localStorage.setItem('theme', newTheme);
             }

             setTheme(theme) {
                 if (theme === 'dark') {
                     document.documentElement.classList.add('dark');
                     if (this.themeToggle) {
                         this.themeToggle.innerHTML = '<i class="fas fa-sun text-gray-600 dark:text-gray-300"></i>';
                     }
                 } else {
                     document.documentElement.classList.remove('dark');
                     if (this.themeToggle) {
                         this.themeToggle.innerHTML = '<i class="fas fa-moon text-gray-600 dark:text-gray-300"></i>';
                     }
                 }
             }
         }

         // 滚动管理
         class ScrollManager {
             constructor() {
                 this.backToTopBtn = document.getElementById('backToTop');
                 this.init();
             }

             init() {
                 this.bindEvents();
                 this.handleScroll();
             }

             bindEvents() {
                 window.addEventListener('scroll', () => this.handleScroll());
                 if (this.backToTopBtn) {
                     this.backToTopBtn.addEventListener('click', () => this.scrollToTop());
                 }
             }

             handleScroll() {
                 const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
                 
                 // 显示/隐藏返回顶部按钮
                 if (this.backToTopBtn) {
                     if (scrollTop > 300) {
                         this.backToTopBtn.classList.add('show');
                     } else {
                         this.backToTopBtn.classList.remove('show');
                     }
                 }
             }

             scrollToTop() {
                 window.scrollTo({
                     top: 0,
                     behavior: 'smooth'
                 });
             }
         }

         // 动画管理
         class AnimationManager {
             constructor() {
                 this.init();
             }

             init() {
                 this.observeElements();
             }

             observeElements() {
                 const observer = new IntersectionObserver((entries) => {
                     entries.forEach(entry => {
                         if (entry.isIntersecting) {
                             entry.target.style.opacity = '1';
                             entry.target.style.transform = 'translateY(0)';
                         }
                     });
                 }, {
                     threshold: 0.1,
                     rootMargin: '0px 0px -50px 0px'
                 });

                 document.querySelectorAll('.fade-in').forEach(el => {
                     observer.observe(el);
                 });
             }
         }

         // 平滑滚动
         function initSmoothScrolling() {
             document.querySelectorAll('a[href^="#"]').forEach(anchor => {
                 anchor.addEventListener('click', function (e) {
                     e.preventDefault();
                     const target = document.querySelector(this.getAttribute('href'));
                     if (target) {
                         target.scrollIntoView({
                             behavior: 'smooth',
                             block: 'start'
                         });
                     }
                 });
             });
         }

         // 初始化所有功能
         document.addEventListener('DOMContentLoaded', function() {
             new ThemeManager();
             new ScrollManager();
             new AnimationManager();
             initSmoothScrolling();
         });
     </script>
 </body>
 </html>